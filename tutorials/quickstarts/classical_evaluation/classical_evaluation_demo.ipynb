{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e18d0e628fdb944",
   "metadata": {},
   "source": [
    "# _Classical_ Evaluations with Scorebook\n",
    "\n",
    "Scorebook, developed by Trismik, is an open-source Python library for model evaluation. It supports both Trismik’s adaptive testing and traditional classical evaluations. In a classical evaluation, a model runs inference on every item in a dataset, and the results are scored using Scorebook’s built-in metrics, such as accuracy, to produce evaluation results.\n",
    "\n",
    "Custom metrics can be implemented and integrated to suit specific evaluation needs.\n",
    "\n",
    "Scorebook also enables evaluation across a grid or list of hyperparameter configurations, streamlining model optimization.\n",
    "\n",
    "Evaluation results can be automatically uploaded to the Scorebook dashboard, organized by project, for storing, managing, and visualizing model evaluation experiments.\n",
    "\n",
    "## Setup Trismik Credentials\n",
    "\n",
    "The results from a classical evaluation can optionally be uploaded to a Trismik project on the dashboard, for the storing, managing, and visualization of evaluation results.\n",
    "\n",
    "If logged in, the `evaluate` function will require a trismik project and experiment id.\n",
    "\n",
    "Enter your trismik API key below."
   ]
  },
  {
   "cell_type": "code",
   "id": "f59fb493a575d361",
   "metadata": {},
   "source": [
    "from scorebook import create_project, login\n",
    "\n",
    "# Set your API key here and run this cell to login\n",
    "TRISMIK_API_KEY = \"your-trismik-api-key-here\"\n",
    "\n",
    "login(TRISMIK_API_KEY)\n",
    "print(\"✓ Logged in to Trismik\")\n",
    "\n",
    "# Create a project\n",
    "project = create_project(\n",
    "    name = \"Classical Evaluation Demo\",\n",
    "    description = \"A project created as part of Trismik's quick-start guides.\"\n",
    ")\n",
    "\n",
    "print(\"✓ Project created\")\n",
    "print(f\"Project ID: {project.id}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cb58f57296229115",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Evaluation Datasets\n",
    "\n",
    "A scorebook evaluation requires an evaluation dataset, represented by the `EvalDataset` class. Evaluation datasets can be constructed via a number of factory methods. In this example we will create a basic evaluation dataset from a list of evaluation items."
   ]
  },
  {
   "cell_type": "code",
   "id": "fa7c936b75c83cad",
   "metadata": {},
   "source": [
    "from scorebook import EvalDataset\n",
    "from scorebook.metrics.accuracy import Accuracy\n",
    "\n",
    "# Create a sample dataset from a list of multiple-choice questions\n",
    "evaluation_items = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"William Shakespeare\"},\n",
    "    {\"question\": \"What is the chemical symbol for gold?\", \"answer\": \"Au\"}\n",
    "]\n",
    "\n",
    "# Create an EvalDataset from the list\n",
    "dataset = EvalDataset.from_list(\n",
    "    name = \"sample_multiple_choice\",\n",
    "    metrics = Accuracy,\n",
    "    items = evaluation_items,\n",
    "    input = \"question\",\n",
    "    label = \"answer\",\n",
    ")\n",
    "\n",
    "print(f\"✓ Created dataset with {len(dataset.items)} items\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "609a95a43d8cfc2c",
   "metadata": {},
   "source": [
    "## Preparing Models for Evaluation\n",
    "\n",
    "To evaluate a model with Scorebook, it must be encapsulated within an inference function. An inference function must accept a list of model inputs, pass these to the model for inference, collect and return outputs generated.\n",
    "\n",
    "### Instantiate a Local Qwen Model\n",
    "\n",
    "For this quick-start guide, we will use the lightweight Qwen2.5 0.5B instruct model, via Hugging Face's transformers package."
   ]
  },
  {
   "cell_type": "code",
   "id": "d1da8af72ef8de6f",
   "metadata": {},
   "source": [
    "import transformers\n",
    "\n",
    "# Instantiate a model\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"✓ Transformers pipeline instantiated\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b56a72374920220",
   "metadata": {},
   "source": [
    "### Define an Inference Function\n",
    "\n",
    "An inference function can be defined to encapsulate any model, local or cloud-hosted. There is flexibility in how an inference function can be defined, the only requirements are the function signature. An inference function must,\n",
    "\n",
    "Accept:\n",
    "\n",
    "- A list of model inputs.\n",
    "- Hyperparameters which can be optionally accessed via kwargs.\n",
    "\n",
    "Return\n",
    "\n",
    "- A list of parsed model outputs for scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "55f6d1eee2fa886e",
   "metadata": {},
   "source": [
    "# Define an inference function\n",
    "from typing import Any, List\n",
    "\n",
    "def qwen(inputs: List[Any], **hyperparameters: Any) -> List[Any]:\n",
    "    \"\"\"Run inference on a list of inputs using the 0.5B Qwen model.\"\"\"\n",
    "    inference_outputs = []\n",
    "\n",
    "    for model_input in inputs:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": hyperparameters.get(\"system_message\", \"You are a helpful assistant.\")},\n",
    "            {\"role\": \"user\", \"content\": str(model_input)},\n",
    "        ]\n",
    "\n",
    "        output = pipeline(\n",
    "            messages,\n",
    "            temperature = hyperparameters.get(\"temperature\", 0.7),\n",
    "            top_p = hyperparameters.get(\"top_p\", 0.9),\n",
    "            top_k = hyperparameters.get(\"top_k\", 50),\n",
    "            max_new_tokens = 512,\n",
    "            do_sample = hyperparameters.get(\"temperature\", 0.7) > 0,\n",
    "        )\n",
    "\n",
    "        inference_outputs.append(output[0][\"generated_text\"][-1][\"content\"])\n",
    "\n",
    "    return inference_outputs\n",
    "\n",
    "print(\"✓ Inference function for Qwen2.5 0.5B defined\")\n",
    "print(qwen([\"Hello!\"]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "693f5e6b94fad17e",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Scorebook makes it easy to evaluate a model across multiple hyperparameter configurations. Hyperparameters evaluated can be specified as\n",
    "\n",
    "1. **A Single Hyperparameter Configuration.**\n",
    "```python\n",
    "hyperparameters = {'temperature': 0.9, 'top_p': 0.8, 'top_k': 40}\n",
    "```\n",
    "\n",
    "2. **A List of Hyperparameter Configurations.**\n",
    "```python\n",
    "hyperparameters = [\n",
    "    {'temperature': 0.9, 'top_p': 0.8, 'top_k': 40},\n",
    "    {'temperature': 0.7, 'top_p': 0.6, 'top_k': 20},\n",
    "]\n",
    "```\n",
    "\n",
    "3. **A Grid of Hyperparameters**\n",
    "\n",
    "```python\n",
    "hyperparameters = {\n",
    "    \"temperature\": [0.3, 0.9],\n",
    "    \"top_p\": [0.6, 0.9],\n",
    "    \"top_k\": [20, 40],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1230b9ad762482af",
   "metadata": {},
   "source": [
    "## Running an Evaluation\n",
    "\n",
    "Running a scorebook evaluation with `evaluate` only requires an inference function and a dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "7031045c655d1062",
   "metadata": {},
   "source": [
    "from scorebook import evaluate\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate(\n",
    "    inference= qwen,\n",
    "    datasets = dataset,\n",
    "    hyperparameters = {\n",
    "        'temperature': 0.9,\n",
    "        'top_p': 0.8,\n",
    "        'top_k': 40,\n",
    "        'system_message': [\"Answer the question directly, provide no additional context.\"]\n",
    "    },\n",
    "    experiment_id = \"Classical-Evaluation-Demo\",\n",
    "    project_id = project.id,\n",
    ")\n",
    "\n",
    "print(\"Qwen2.5 0.5B Evaluation Results:\")\n",
    "print(f\"accuracy: {results[0]['accuracy']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Next Steps\n",
    "\n",
    "- [Scorebook docs](https://docs.trismik.com/scorebook/introduction-to-scorebook/): Scorebook's full documentation.\n",
    "- [Scorebook repository](https://github.com/trismik/scorebook): Scorebook is an open-source library, view the code and more examples."
   ],
   "id": "61d9ff67d63624d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
