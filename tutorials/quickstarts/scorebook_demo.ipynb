{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# _Classic_ Evaluations with Scorebook\n",
    "\n",
    "Scorebook, developed by Trismik, is an open-source Python library for model evaluation. It supports both Trismik’s adaptive testing and traditional classical evaluations. In a classical evaluation, a model runs inference on every item in a dataset, and the results are scored using Scorebook’s built-in metrics — including accuracy, precision, recall, ROUGE, BLEU, and BERT — to produce a complete performance report.\n",
    "\n",
    "Custom metrics can be implemented and integrated to suit specific evaluation needs.\n",
    "\n",
    "Scorebook also enables evaluation across a grid or list of hyperparameter configurations, streamlining model optimization.\n",
    "\n",
    "Evaluation results can be automatically uploaded to the Scorebook dashboard, organized by project, for storing, managing, and visualizing model evaluation experiments.\n",
    "\n",
    "## Evaluation Datasets\n",
    "\n",
    "A scorebook evaluation requires an evaluation dataset, represented by the `EvalDataset` class. Evaluation datasets can be constructed via a number of factory methods. In this example we will create a basic evaluation dataset from a list of evaluation items."
   ],
   "id": "cb58f57296229115"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scorebook import EvalDataset\n",
    "from scorebook.metrics.accuracy import Accuracy\n",
    "\n",
    "# Create a sample dataset from a list of multiple-choice questions\n",
    "evaluation_items = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"William Shakespeare\"},\n",
    "]\n",
    "\n",
    "# Create an EvalDataset from the list\n",
    "dataset = EvalDataset.from_list(\n",
    "    name = \"sample_multiple_choice\",\n",
    "    metrics = Accuracy,\n",
    "    items = evaluation_items,\n",
    "    input = \"question\",\n",
    "    label = \"answer\",\n",
    ")\n",
    "\n",
    "print(f\"Created dataset with {len(dataset.items)} items\")"
   ],
   "id": "fa7c936b75c83cad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Inference Functions\n",
    "\n",
    "To evaluate a model with Scorebook, it must be encapsulated within an inference function. An inference function must accept a list of model inputs, pass these to the model for inference, collect and return outputs generated.\n",
    "\n",
    "For this example, we will use the Microsoft Phi-4 Mini Instruct model, via Hugging Face's transformers package."
   ],
   "id": "609a95a43d8cfc2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import transformers\n",
    "\n",
    "# Instantiate a model\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/Phi-4-mini-instruct\",\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "    device_map=\"auto\"\n",
    ")"
   ],
   "id": "d1da8af72ef8de6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3b56a72374920220"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import Any, List\n",
    "\n",
    "# Define an inference function\n",
    "def inference_function(inputs: List[Any], **hyperparameters: Any) -> List[Any]:\n",
    "    \"\"\"Process a list of inputs through a model.\n",
    "\n",
    "    Args:\n",
    "        inputs: List of input values from the dataset\n",
    "        hyperparameters: Model hyperparameters (passed automatically by Scorebook)\n",
    "\n",
    "    Returns:\n",
    "        List of model outputs (predictions)\n",
    "    \"\"\"\n",
    "    inference_outputs = []\n",
    "    for model_input in inputs:\n",
    "\n",
    "        # Wrap inputs in the model's message format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": hyperparameters.get(\"system_message\"),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": model_input},\n",
    "        ]\n",
    "\n",
    "        # Run inference on the item\n",
    "        output = pipeline(messages, temperature=hyperparameters.get(\"temperature\"))\n",
    "\n",
    "        # Extract and collect the output generated from the model's response\n",
    "        inference_outputs.append(output[0][\"generated_text\"][-1][\"content\"])\n",
    "\n",
    "    return inference_outputs"
   ],
   "id": "55f6d1eee2fa886e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter Sweeps",
   "id": "693f5e6b94fad17e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define hyperparameters with lists to create a grid search\n",
    "hyperparameters = {\n",
    "    \"temperature\": [0.6, 0.7, 0.8, 0.9],\n",
    "    \"top_p\": [0.8, 0.9],\n",
    "}"
   ],
   "id": "d0e87dbba85c51b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Running an Evaluation",
   "id": "1230b9ad762482af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scorebook import evaluate\n",
    "\n",
    "# Run evaluation across all hyperparameter combinations\n",
    "results = evaluate(\n",
    "    inference_function,                # Your inference function\n",
    "    dataset,                           # Your evaluation dataset\n",
    "    hyperparameters=hyperparameters,   # Hyperparameter grid\n",
    ")"
   ],
   "id": "7031045c655d1062"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation Results",
   "id": "16be2dd27a05afef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
