{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3ba3cd77800bb4",
   "metadata": {},
   "source": [
    "# Adaptive Evaluations with Scorebook - Evaluating a Local Qwen Model\n",
    "\n",
    "This quick-start guide showcases an adaptive evaluation of Qwen's Qwen2.5 0.5B Instruct model.\n",
    "\n",
    "We recommend that you first see our [getting started quick-start guide](https://colab.research.google.com/github/trismik/scorebook/blob/main/tutorials/quickstarts/getting_started.ipynb) if you have not done so already, for more of a detailed overview on adaptive testing and setting up Trismik credentials.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Trismik API key**: Generate a Trismik API key from the [Trismik dashboard's settings page](https://app.trismik.com/settings).\n",
    "- **Trismik Project Id**: We recommend you use the project id generated in the [Getting Started Quick-Start Guide](https://colab.research.google.com/github/trismik/scorebook/blob/main/tutorials/quickstarts/getting_started.ipynb).\n",
    "\n",
    "## Install Scorebook"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install scorebook\n",
    "# if you're running this locally, please run !pip install scorebook\"[examples]\""
   ],
   "id": "90146caef86f19ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cad992b287d4d0ac",
   "metadata": {},
   "source": [
    "## Setup Credentials\n",
    "\n",
    "Enter your Trismik API key and project id below."
   ]
  },
  {
   "cell_type": "code",
   "id": "14e576282749edb7",
   "metadata": {},
   "source": [
    "# Set your credentials here\n",
    "TRISMIK_API_KEY = \"your-trismik-api-key-here\"\n",
    "TRISMIK_PROJECT_ID = \"your-trismik-project-id-here\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "700950d039e4c0f6",
   "metadata": {},
   "source": [
    "## Login with Trismik API Key"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "from scorebook import login\n",
    "\n",
    "# Login to Trismik\n",
    "login(TRISMIK_API_KEY)\n",
    "print(\"✓ Logged in to Trismik\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "609a95a43d8cfc2c",
   "metadata": {},
   "source": [
    "## Instantiate a Local Qwen Model\n",
    "\n",
    "For this quick-start guide, we will use the lightweight Qwen2.5 0.5B instruct model, via Hugging Face's transformers package."
   ]
  },
  {
   "cell_type": "code",
   "id": "d1da8af72ef8de6f",
   "metadata": {},
   "source": [
    "import transformers\n",
    "\n",
    "# Instantiate a model\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"✓ Transformers pipeline instantiated\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13084db21e549ccf",
   "metadata": {},
   "source": "## Define Inference Functions\n\nTo evaluate a model with Scorebook, it must be encapsulated within an inference function. An inference function must accept a list of model inputs, pass these to the model for inference, collect and return outputs generated.\n\nAn inference function can be defined to encapsulate any model, local or cloud-hosted. There is flexibility in how an inference function can be defined, the only requirements are the function signature. An inference function must,\n\nAccept:\n\n- A list of model inputs.\n- Hyperparameters which can be optionally accessed via kwargs.\n\nReturn\n\n- A list of parsed model outputs for scoring.\n\nWe define two separate inference functions: one for multiple-choice items and one for open-ended items."
  },
  {
   "cell_type": "code",
   "id": "8aa99f513db6241a",
   "metadata": {},
   "source": "from typing import Any, List\n\n\ndef mc_inference(inputs: List[Any], **hyperparameters: Any) -> List[Any]:\n    \"\"\"Process multiple-choice inputs through Qwen model.\"\"\"\n    outputs = []\n    for input_val in inputs:\n        choices = input_val.get(\"choices\", [])\n        prompt = (\n            str(input_val.get(\"question\", \"\"))\n            + \"\\nOptions:\\n\"\n            + \"\\n\".join(f\"{choice['id']}: {choice['text']}\" for choice in choices)\n        )\n\n        messages = [\n            {\"role\": \"system\", \"content\": \"Answer with only the letter of the correct option.\"},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        try:\n            output = pipeline(\n                messages,\n                temperature=0.7,\n                top_p=0.9,\n                top_k=50,\n                max_new_tokens=512,\n                max_length=None,\n                do_sample=True,\n            )\n            response = output[0][\"generated_text\"][-1][\"content\"].strip()\n        except Exception as e:\n            response = f\"Error: {str(e)}\"\n\n        outputs.append(response)\n    return outputs\n\n\ndef open_ended_inference(inputs: List[Any], **hyperparameters: Any) -> List[Any]:\n    \"\"\"Process open-ended inputs through Qwen model.\"\"\"\n    outputs = []\n    for input_val in inputs:\n        prompt = str(input_val.get(\"question\", \"\"))\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"Answer the question. Place your final answer between <answer> and </answer> tags.\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        try:\n            output = pipeline(\n                messages,\n                temperature=0.7,\n                top_p=0.9,\n                top_k=50,\n                max_new_tokens=512,\n                max_length=None,\n                do_sample=True,\n            )\n            response = output[0][\"generated_text\"][-1][\"content\"].strip()\n\n            # Extract from <answer> tags if present\n            start = response.rfind(\"<answer>\")\n            end = response.rfind(\"</answer>\")\n            if start != -1 and end > start:\n                response = response[start + len(\"<answer>\"):end].strip()\n\n        except Exception as e:\n            response = f\"Error: {str(e)}\"\n\n        outputs.append(response)\n    return outputs",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "efa5c3ea791bbcd1",
   "metadata": {},
   "source": "## Run an Adaptive Evaluation\n\nWhen running an adaptive evaluation, we can use any single or multiple adaptive datasets and specify a split to be evaluated.\n\n### Multiple-Choice Adaptive Evaluation"
  },
  {
   "cell_type": "code",
   "id": "3cbf1b2f13d5553e",
   "metadata": {},
   "source": "from scorebook import evaluate\n\n# Run multiple-choice adaptive evaluation\nresults = evaluate(\n    inference=mc_inference,\n    datasets=\"trismik/figQA:adaptive\",\n    split=\"validation\",\n    experiment_id=\"Adaptive Evaluation Tutorial\",\n    project_id=TRISMIK_PROJECT_ID,\n)\n\nprint(\"Adaptive evaluation complete!\")\nprint(\"Results: \", results[0][\"score\"])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "54ni33xdayv",
   "source": "### Open-Ended Adaptive Evaluation\n\nScorebook also supports open-ended adaptive evaluations where the model provides free-text answers instead of selecting from multiple choices. We use a separate inference function tailored for open-ended items.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zdto899thl",
   "source": "# Run open-ended adaptive evaluation\nresults_open_ended = evaluate(\n    inference=open_ended_inference,\n    datasets=\"trismik/fingpt_convfinqa_test:adaptive\",\n    split=\"validation\",\n    experiment_id=\"Adaptive Evaluation Tutorial\",\n    project_id=TRISMIK_PROJECT_ID,\n)\n\nprint(\"Open-ended adaptive evaluation complete!\")\nprint(\"Results: \", results_open_ended[0][\"score\"])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d37cb5e87cc297fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Adaptive Testing White Paper](https://docs.trismik.com/adaptiveTesting/adaptive-testing-introduction/): An in depth overview of the science behind the adaptive testing methodology.\n",
    "- [Dataset Page](https://dashboard.trismik.com/datasets): Trismik's full set of currently adaptive datasets from the Trismik dashboard.\n",
    "- [Scorebook Docs](https://docs.trismik.com/scorebook/introduction-to-scorebook/): Scorebook's full documentation.\n",
    "- [Scorebook Repository](https://github.com/trismik/scorebook): Scorebook is an open-source library, view the code and more examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}