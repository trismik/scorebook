{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3ba3cd77800bb4",
   "metadata": {},
   "source": [
    "# Adaptive Evaluations with Scorebook - Evaluating an OpenAI GPT Model\n",
    "\n",
    "This quick-start guide showcases an adaptive evaluation of OpenAI's GPT-4o Mini model.\n",
    "\n",
    "We recommend that you first see our [getting started quick-start guide](https://colab.research.google.com/github/trismik/scorebook/blob/main/tutorials/quickstarts/getting_started.ipynb) if you have not done so already, for more of a detailed overview on adaptive testing and setting up Trismik credentials.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Trismik API key**: Generate a Trismik API key from the [Trismik dashboard's settings page](https://app.trismik.com/settings).\n",
    "- **Trismik Project Id**: We recommend you use the project id generated in the [Getting Started Quick-Start Guide](https://colab.research.google.com/github/trismik/scorebook/blob/main/tutorials/quickstarts/getting_started.ipynb).\n",
    "- **OpenAI API key**: Generate an OpenAI API key from [OpenAI's API Platform](https://openai.com/api/).\n",
    "\n",
    "## Install Scorebook"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install scorebook\n",
    "# if you're running this locally, please run !pip install scorebook\"[examples, providers]\""
   ],
   "id": "f454e876551a4a0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Setup Credentials\n",
    "\n",
    "Enter your Trismik API key, project id and OpenAI API Key below."
   ],
   "id": "cad992b287d4d0ac"
  },
  {
   "cell_type": "code",
   "id": "14e576282749edb7",
   "metadata": {},
   "source": [
    "# Set your credentials here\n",
    "TRISMIK_API_KEY = \"your-trismik-api-key-here\"\n",
    "TRISMIK_PROJECT_ID = \"your-trismik-project-id-here\"\n",
    "OPENAI_API_KEY = \"your-openai-api-key-here\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "700950d039e4c0f6",
   "metadata": {},
   "source": [
    "## Login with Trismik API Key"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "from scorebook import login\n",
    "\n",
    "# Login to Trismik\n",
    "login(TRISMIK_API_KEY)\n",
    "print(\"✓ Logged in to Trismik\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13084db21e549ccf",
   "metadata": {},
   "source": [
    "## Define an Inference Function\n",
    "\n",
    "To evaluate a model with Scorebook, it must be encapsulated within an inference function. An inference function must accept a list of model inputs, pass these to the model for inference, collect and return outputs generated.\n",
    "\n",
    "An inference function can be defined to encapsulate any model, local or cloud-hosted. There is flexibility in how an inference function can be defined, the only requirements are the function signature. An inference function must,\n",
    "\n",
    "Accept:\n",
    "\n",
    "- A list of model inputs.\n",
    "- Hyperparameters which can be optionally accessed via kwargs.\n",
    "\n",
    "Return\n",
    "\n",
    "- A list of parsed model outputs for scoring."
   ]
  },
  {
   "cell_type": "code",
   "id": "8aa99f513db6241a",
   "metadata": {},
   "source": "from openai import OpenAI\nfrom typing import Any, List\n\nclient = OpenAI(api_key=OPENAI_API_KEY)\n\n# define an inference function for GPT-4o Mini.\ndef gpt4o_mini(inputs: List[Any], **hyperparameters: Any) -> List[Any]:\n    \"\"\"Process inputs through OpenAI's API. Handles both multiple-choice and open-ended items.\"\"\"\n\n    outputs = []\n    for idx, input_item in enumerate(inputs):\n\n        choices = input_item.get(\"choices\")\n\n        if choices:\n            # Multiple-choice: format with choices, ask for letter\n            prompt = (\n                str(input_item.get(\"question\", \"\"))\n                + \"\\nOptions:\\n\"\n                + \"\\n\".join(\n                    f\"{choice['id']}: {choice['text']}\"\n                    for choice in choices\n                )\n            )\n            system_msg = hyperparameters.get(\n                \"system_message\",\n                \"Answer with only the letter of the correct option.\"\n            )\n        else:\n            # Open-ended: just the question, ask for free text\n            prompt = str(input_item.get(\"question\", \"\"))\n            system_msg = hyperparameters.get(\n                \"system_message_open_ended\",\n                \"Answer the question. Place your final answer between <answer> and </answer> tags.\"\n            )\n\n        # Build messages for OpenAI API\n        messages = [\n            {\"role\": \"system\", \"content\": system_msg},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        # Call OpenAI API and extract output from the response\n        try:\n            response = client.chat.completions.create(\n                model=\"gpt-4o-mini\",\n                messages=messages,\n                temperature=0.7,\n            )\n            output = response.choices[0].message.content.strip()\n\n            # For open-ended: extract from <answer> tags if present\n            if not choices and \"<answer>\" in output:\n                start = output.rfind(\"<answer>\") + len(\"<answer>\")\n                end = output.rfind(\"</answer>\")\n                if end > start:\n                    output = output[start:end].strip()\n\n        except Exception as e:\n            output = f\"Error: {str(e)}\"\n\n        outputs.append(output)\n\n    return outputs",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "efa5c3ea791bbcd1",
   "metadata": {},
   "source": "## Run an Adaptive Evaluation\n\nWhen running an adaptive evaluation, we can use any single or multiple adaptive datasets and specify a split to be evaluated.\n\n### Multiple-Choice Adaptive Evaluation"
  },
  {
   "cell_type": "code",
   "id": "3cbf1b2f13d5553e",
   "metadata": {},
   "source": [
    "from scorebook import evaluate\n",
    "\n",
    "# Run adaptive evaluation\n",
    "results = evaluate(\n",
    "    inference = gpt4o_mini,\n",
    "    datasets = \"trismik/figQA:adaptive\",\n",
    "    hyperparameters = {\"system_message\": \"Answer the question with only the letter of the correct option. No additional text or context\"},\n",
    "    split = \"validation\",\n",
    "    experiment_id = \"GPT-4o-Mini-Adaptive-Evaluation\",\n",
    "    project_id = TRISMIK_PROJECT_ID,\n",
    ")\n",
    "\n",
    "# Print the adaptive evaluation results\n",
    "print(\"✓ Adaptive evaluation complete!\")\n",
    "print(\"Results: \", results[0][\"score\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "rnm6jhr3ffb",
   "source": "### Open-Ended Adaptive Evaluation\n\nScorebook also supports open-ended adaptive evaluations where the model provides free-text answers instead of selecting from multiple choices. The same inference function handles both item types by checking for the presence of `choices` in the input item.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "m8zh3arwbxo",
   "source": "# Run open-ended adaptive evaluation\nresults_open_ended = evaluate(\n    inference=gpt4o_mini,\n    datasets=\"fingpt_convfinqa_test:adaptive\",\n    hyperparameters={\n        \"system_message\": \"Answer with only the letter of the correct option.\",\n        \"system_message_open_ended\": \"Answer the question concisely. Place your final answer between <answer> and </answer> tags.\",\n    },\n    split=\"validation\",\n    experiment_id=\"GPT-4o-Mini-Open-Ended-Adaptive\",\n    project_id=TRISMIK_PROJECT_ID,\n)\n\n# Print the open-ended adaptive evaluation results\nprint(\"✓ Open-ended adaptive evaluation complete!\")\nprint(\"Results: \", results_open_ended[0][\"score\"])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d37cb5e87cc297fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Adaptive Testing White Paper](https://docs.trismik.com/adaptiveTesting/adaptive-testing-introduction/): An in depth overview of the science behind the adaptive testing methodology.\n",
    "- [Dataset Page](https://dashboard.trismik.com/datasets): Trismik's full set of currently adaptive datasets from the Trismik dashboard.\n",
    "- [Scorebook Docs](https://docs.trismik.com/scorebook/introduction-to-scorebook/): Scorebook's full documentation.\n",
    "- [Scorebook Repository](https://github.com/trismik/scorebook): Scorebook is an open-source library, view the code and more examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}