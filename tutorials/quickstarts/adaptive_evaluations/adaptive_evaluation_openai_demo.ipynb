{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3ba3cd77800bb4",
   "metadata": {},
   "source": [
    "# Adaptive Evaluations with Scorebook - Evaluating an OpenAI GPT Model\n",
    "\n",
    "This quick-start guide showcases an adaptive evaluation of OpenAI's GPT-4o Mini model.\n",
    "\n",
    "We recommend that you first see our [getting started quick-start guide](https://colab.research.google.com/github/trismik/scorebook/blob/main/tutorials/quickstarts/getting_started.ipynb) if you have not done so already, for more of a detailed overview on adaptive testing and setting up Trismik credentials.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Trismik API key**: Generate a Trismik API key from the [Trismik dashboard's settings page](https://app.trismik.com/settings).\n",
    "- **Trismik Project Id**: We recommend you use the project id generated in the [Getting Started Quick-Start Guide](https://colab.research.google.com/github/trismik/scorebook/blob/main/tutorials/quickstarts/getting_started.ipynb).\n",
    "- **OpenAI API key**: Generate an OpenAI API key from [OpenAI's API Platform](https://openai.com/api/).\n",
    "\n",
    "## Install Scorebook"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install scorebook\n",
    "# if you're running this locally, please run !pip install scorebook\"[examples, providers]\""
   ],
   "id": "f454e876551a4a0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Setup Credentials\n",
    "\n",
    "Enter your Trismik API key, project id and OpenAI API Key below."
   ],
   "id": "cad992b287d4d0ac"
  },
  {
   "cell_type": "code",
   "id": "14e576282749edb7",
   "metadata": {},
   "source": [
    "# Set your credentials here\n",
    "TRISMIK_API_KEY = \"your-trismik-api-key-here\"\n",
    "TRISMIK_PROJECT_ID = \"your-trismik-project-id-here\"\n",
    "OPENAI_API_KEY = \"your-openai-api-key-here\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "700950d039e4c0f6",
   "metadata": {},
   "source": [
    "## Login with Trismik API Key"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "from scorebook import login\n",
    "\n",
    "# Login to Trismik\n",
    "login(TRISMIK_API_KEY)\n",
    "print(\"âœ“ Logged in to Trismik\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13084db21e549ccf",
   "metadata": {},
   "source": "## Define Inference Functions\n\nTo evaluate a model with Scorebook, it must be encapsulated within an inference function. An inference function must accept a list of model inputs, pass these to the model for inference, collect and return outputs generated.\n\nAn inference function can be defined to encapsulate any model, local or cloud-hosted. There is flexibility in how an inference function can be defined, the only requirements are the function signature. An inference function must,\n\nAccept:\n\n- A list of model inputs.\n- Hyperparameters which can be optionally accessed via kwargs.\n\nReturn\n\n- A list of parsed model outputs for scoring.\n\nWe define two separate inference functions: one for multiple-choice items and one for open-ended items."
  },
  {
   "cell_type": "code",
   "id": "8aa99f513db6241a",
   "metadata": {},
   "source": "from openai import OpenAI\nfrom typing import Any, List\n\nclient = OpenAI(api_key=OPENAI_API_KEY)\nmodel_name = \"gpt-4o-mini\"\n\n\ndef mc_inference(inputs: List[Any], **hyperparameters: Any) -> List[Any]:\n    \"\"\"Process multiple-choice inputs through OpenAI's API.\"\"\"\n    outputs = []\n    for input_val in inputs:\n        choices = input_val.get(\"choices\", [])\n        prompt = (\n            str(input_val.get(\"question\", \"\"))\n            + \"\\nOptions:\\n\"\n            + \"\\n\".join(f\"{choice['id']}: {choice['text']}\" for choice in choices)\n        )\n\n        messages = [\n            {\"role\": \"system\", \"content\": \"Answer with only the letter of the correct option.\"},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        try:\n            response = client.chat.completions.create(\n                model=model_name, messages=messages, temperature=0.7,\n            )\n            output = response.choices[0].message.content.strip()\n        except Exception as e:\n            output = f\"Error: {str(e)}\"\n\n        outputs.append(output)\n    return outputs\n\n\ndef open_ended_inference(inputs: List[Any], **hyperparameters: Any) -> List[Any]:\n    \"\"\"Process open-ended inputs through OpenAI's API.\"\"\"\n    outputs = []\n    for input_val in inputs:\n        prompt = str(input_val.get(\"question\", \"\"))\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"Answer the question. Place your final answer between <answer> and </answer> tags.\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n\n        try:\n            response = client.chat.completions.create(\n                model=model_name, messages=messages, temperature=0.7,\n            )\n            output = response.choices[0].message.content.strip()\n\n            # Extract from <answer> tags if present\n            start = output.rfind(\"<answer>\")\n            end = output.rfind(\"</answer>\")\n            if start != -1 and end > start:\n                output = output[start + len(\"<answer>\"):end].strip()\n\n        except Exception as e:\n            output = f\"Error: {str(e)}\"\n\n        outputs.append(output)\n    return outputs",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "efa5c3ea791bbcd1",
   "metadata": {},
   "source": "## Run an Adaptive Evaluation\n\nWhen running an adaptive evaluation, we can use any single or multiple adaptive datasets and specify a split to be evaluated.\n\n### Multiple-Choice Adaptive Evaluation"
  },
  {
   "cell_type": "code",
   "id": "3cbf1b2f13d5553e",
   "metadata": {},
   "source": "from scorebook import evaluate\n\n# Run multiple-choice adaptive evaluation\nresults = evaluate(\n    inference=mc_inference,\n    datasets=\"trismik/figQA:adaptive\",\n    split=\"validation\",\n    experiment_id=\"Adaptive Evaluation Tutorial\",\n    project_id=TRISMIK_PROJECT_ID,\n)\n\nprint(\"Adaptive evaluation complete!\")\nprint(\"Results: \", results[0][\"score\"])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "rnm6jhr3ffb",
   "source": "### Open-Ended Adaptive Evaluation\n\nScorebook also supports open-ended adaptive evaluations where the model provides free-text answers instead of selecting from multiple choices. We use a separate inference function tailored for open-ended items.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "m8zh3arwbxo",
   "source": "# Run open-ended adaptive evaluation\nresults_open_ended = evaluate(\n    inference=open_ended_inference,\n    datasets=\"trismik/fingpt_convfinqa_test:adaptive\",\n    split=\"validation\",\n    experiment_id=\"Adaptive Evaluation Tutorial\",\n    project_id=TRISMIK_PROJECT_ID,\n)\n\nprint(\"Open-ended adaptive evaluation complete!\")\nprint(\"Results: \", results_open_ended[0][\"score\"])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d37cb5e87cc297fe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Adaptive Testing White Paper](https://docs.trismik.com/adaptiveTesting/adaptive-testing-introduction/): An in depth overview of the science behind the adaptive testing methodology.\n",
    "- [Dataset Page](https://dashboard.trismik.com/datasets): Trismik's full set of currently adaptive datasets from the Trismik dashboard.\n",
    "- [Scorebook Docs](https://docs.trismik.com/scorebook/introduction-to-scorebook/): Scorebook's full documentation.\n",
    "- [Scorebook Repository](https://github.com/trismik/scorebook): Scorebook is an open-source library, view the code and more examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}