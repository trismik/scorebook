{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Results to Trismik Dashboard\n",
    "\n",
    "This notebook demonstrates three ways to upload evaluation results to Trismik's dashboard for tracking and visualization.\n",
    "\n",
    "## Why Upload Results?\n",
    "\n",
    "- **Track Progress**: Monitor model performance over time\n",
    "- **Compare Models**: Visualize performance across different models and experiments\n",
    "- **Share Results**: Collaborate with your team on evaluation insights\n",
    "- **Historical Analysis**: Maintain a record of all evaluations\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Trismik API key**: Get yours at https://app.trismik.com/settings\n",
    "- **Trismik Project ID**: Create a project at https://app.trismik.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Set your API credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your API keys here or load from .env file\n",
    "os.environ[\"TRISMIK_API_KEY\"] = \"your-trismik-api-key\"\n",
    "os.environ[\"TRISMIK_PROJECT_ID\"] = \"your-project-id\"\n",
    "\n",
    "# Or load from .env file\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from scorebook import score, login\n",
    "from scorebook.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to Trismik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get(\"TRISMIK_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"TRISMIK_API_KEY not set. Get your API key from https://app.trismik.com/settings\")\n",
    "\n",
    "login(api_key)\n",
    "print(\"âœ“ Logged in to Trismik\")\n",
    "\n",
    "project_id = os.environ.get(\"TRISMIK_PROJECT_ID\")\n",
    "if not project_id:\n",
    "    raise ValueError(\"TRISMIK_PROJECT_ID not set. Find your project ID at https://app.trismik.com\")\n",
    "\n",
    "print(f\"âœ“ Using project: {project_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Upload score() Results\n",
    "\n",
    "Score pre-computed outputs and upload to Trismik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare items with pre-computed outputs\n",
    "items = [\n",
    "    {\"input\": \"What is 2 + 2?\", \"output\": \"4\", \"label\": \"4\"},\n",
    "    {\"input\": \"What is the capital of France?\", \"output\": \"Paris\", \"label\": \"Paris\"},\n",
    "    {\"input\": \"Who wrote Romeo and Juliet?\", \"output\": \"William Shakespeare\", \"label\": \"William Shakespeare\"},\n",
    "    {\"input\": \"What is 5 * 6?\", \"output\": \"30\", \"label\": \"30\"},\n",
    "    {\"input\": \"What is the largest planet?\", \"output\": \"Jupiter\", \"label\": \"Jupiter\"},\n",
    "]\n",
    "\n",
    "# Score and upload\n",
    "results = score(\n",
    "    items=items,\n",
    "    metrics=Accuracy,\n",
    "    dataset_name=\"basic_questions\",\n",
    "    model_name=\"example-model-v1\",\n",
    "    experiment_id=\"Score-Upload-Notebook\",\n",
    "    project_id=project_id,\n",
    "    metadata={\n",
    "        \"description\": \"Example from Jupyter notebook\",\n",
    "        \"note\": \"Pre-computed outputs uploaded via score()\",\n",
    "    },\n",
    "    upload_results=True,  # Enable uploading\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Results uploaded successfully!\")\n",
    "print(f\"Accuracy: {results['aggregates']['Accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Upload evaluate() Results\n",
    "\n",
    "Run inference and automatically upload results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "from scorebook import EvalDataset, evaluate\n",
    "\n",
    "# Create a simple dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "sample_data = [\n",
    "    {\"question\": \"What is 10 + 5?\", \"answer\": \"15\"},\n",
    "    {\"question\": \"What is the capital of Spain?\", \"answer\": \"Madrid\"},\n",
    "]\n",
    "\n",
    "temp_file = Path(\"temp_eval_dataset.json\")\n",
    "with open(temp_file, \"w\") as f:\n",
    "    json.dump(sample_data, f)\n",
    "\n",
    "dataset = EvalDataset.from_json(\n",
    "    path=str(temp_file),\n",
    "    metrics=\"accuracy\",\n",
    "    input=\"question\",\n",
    "    label=\"answer\",\n",
    ")\n",
    "\n",
    "# Define a simple inference function (mock)\n",
    "def mock_inference(inputs: List[Any], **hyperparameters: Any) -> List[Any]:\n",
    "    \"\"\"Mock inference that returns the expected answers.\"\"\"\n",
    "    # In practice, this would call your model\n",
    "    return [\"15\", \"Madrid\"]  # Mock perfect answers\n",
    "\n",
    "# Run evaluation with upload\n",
    "eval_results = evaluate(\n",
    "    mock_inference,\n",
    "    dataset,\n",
    "    hyperparameters={\"temperature\": 0.7},\n",
    "    experiment_id=\"Evaluate-Upload-Notebook\",\n",
    "    project_id=project_id,\n",
    "    metadata={\n",
    "        \"model\": \"mock-model\",\n",
    "        \"description\": \"Evaluation results from notebook\",\n",
    "    },\n",
    "    return_aggregates=True,\n",
    "    return_items=True,\n",
    "    return_output=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Evaluation results uploaded!\")\n",
    "print(f\"Accuracy: {eval_results['aggregates']['accuracy']:.2%}\")\n",
    "\n",
    "# Cleanup\n",
    "temp_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Upload External Results\n",
    "\n",
    "Import results from external evaluation frameworks or historical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Import results from another evaluation framework\n",
    "external_results = [\n",
    "    {\"input\": \"Translate 'hello' to Spanish\", \"output\": \"hola\", \"label\": \"hola\"},\n",
    "    {\"input\": \"Translate 'goodbye' to Spanish\", \"output\": \"adiÃ³s\", \"label\": \"adiÃ³s\"},\n",
    "    {\"input\": \"Translate 'thank you' to Spanish\", \"output\": \"gracias\", \"label\": \"gracias\"},\n",
    "    {\"input\": \"Translate 'please' to Spanish\", \"output\": \"por favor\", \"label\": \"por favor\"},\n",
    "]\n",
    "\n",
    "# Upload external results\n",
    "external_upload = score(\n",
    "    items=external_results,\n",
    "    metrics=\"accuracy\",\n",
    "    dataset_name=\"spanish_translation\",\n",
    "    model_name=\"external-translator-v2\",\n",
    "    experiment_id=\"External-Results-Upload\",\n",
    "    project_id=project_id,\n",
    "    metadata={\n",
    "        \"description\": \"Historical results imported from external framework\",\n",
    "        \"source\": \"Custom evaluation pipeline\",\n",
    "        \"date\": \"2025-01-15\",\n",
    "    },\n",
    "    upload_results=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ External results uploaded!\")\n",
    "print(f\"Accuracy: {external_upload['aggregates']['accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results on Dashboard\n",
    "\n",
    "All uploaded results are now visible on your Trismik dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "dashboard_url = f\"https://app.trismik.com/projects/{project_id}\"\n",
    "display(Markdown(f\"### ðŸ“Š [View All Results on Dashboard]({dashboard_url})\"))\n",
    "print(f\"\\nDirect link: {dashboard_url}\")\n",
    "print(\"\\nYou should see three experiments:\")\n",
    "print(\"  1. Score-Upload-Notebook\")\n",
    "print(\"  2. Evaluate-Upload-Notebook\")\n",
    "print(\"  3. External-Results-Upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Results with Metadata\n",
    "\n",
    "Use metadata to add context and organization to your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Organizing a model comparison experiment\n",
    "models_to_test = [\n",
    "    {\"name\": \"model-a\", \"version\": \"1.0\"},\n",
    "    {\"name\": \"model-b\", \"version\": \"2.0\"},\n",
    "]\n",
    "\n",
    "test_items = [\n",
    "    {\"output\": \"positive\", \"label\": \"positive\"},\n",
    "    {\"output\": \"negative\", \"label\": \"negative\"},\n",
    "]\n",
    "\n",
    "for model_info in models_to_test:\n",
    "    result = score(\n",
    "        items=test_items,\n",
    "        metrics=Accuracy,\n",
    "        dataset_name=\"sentiment_test\",\n",
    "        model_name=model_info[\"name\"],\n",
    "        experiment_id=\"Model-Comparison-Notebook\",\n",
    "        project_id=project_id,\n",
    "        metadata={\n",
    "            \"model_version\": model_info[\"version\"],\n",
    "            \"comparison_group\": \"sentiment_analysis\",\n",
    "            \"date\": \"2025-01-26\",\n",
    "            \"notes\": f\"Testing {model_info['name']} v{model_info['version']}\",\n",
    "        },\n",
    "        upload_results=True,\n",
    "    )\n",
    "    print(f\"âœ“ Uploaded results for {model_info['name']} v{model_info['version']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Experiment Naming\n",
    "- Use descriptive `experiment_id` values (e.g., \"GPT4-MMLU-Baseline\")\n",
    "- Group related runs under the same experiment ID\n",
    "- Use different experiment IDs for different types of tests\n",
    "\n",
    "### Metadata\n",
    "- Include model version, hyperparameters, and configuration\n",
    "- Add timestamps and descriptions for historical tracking\n",
    "- Use consistent keys across experiments for easy comparison\n",
    "\n",
    "### Organization\n",
    "- Create separate projects for different use cases\n",
    "- Use tags or metadata fields to categorize experiments\n",
    "- Document your evaluation methodology in metadata\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore the Trismik dashboard to visualize trends and comparisons\n",
    "- Set up automated evaluation pipelines with result uploading\n",
    "- Try the **Adaptive Evaluations** notebook for efficient testing with automatic uploads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
