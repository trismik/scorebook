{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Uploading Results to Trismik Dashboard\n\nThis notebook demonstrates three ways to upload evaluation results to Trismik's dashboard for tracking and visualization.\n\n## Why Upload Results?\n\n- **Track Progress**: Monitor model performance over time\n- **Compare Models**: Visualize performance across different models and experiments\n- **Share Results**: Collaborate with your team on evaluation insights\n- **Historical Analysis**: Maintain a record of all evaluations\n\n## Prerequisites\n\n- **Trismik API key**: Get yours at https://app.trismik.com/settings\n- **Trismik Project**: Create a project at https://app.trismik.com and copy its Project ID"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup Credentials\n\nSet your Trismik credentials here:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# STEP 1: Get your Trismik API key from https://app.trismik.com/settings\n# STEP 2: Create a project at https://app.trismik.com and copy the Project ID\n\n# Set your credentials here\nTRISMIK_API_KEY = \"your-trismik-api-key\"  # pragma: allowlist secret\nTRISMIK_PROJECT_ID = \"your-project-id\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from scorebook import score, login\n",
    "from scorebook.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to Trismik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not TRISMIK_API_KEY or TRISMIK_API_KEY == \"your-trismik-api-key\":\n    raise ValueError(\"Please set TRISMIK_API_KEY. Get your API key from https://app.trismik.com/settings\")\n\nlogin(TRISMIK_API_KEY)\nprint(\"âœ“ Logged in to Trismik\")\n\nif not TRISMIK_PROJECT_ID or TRISMIK_PROJECT_ID == \"your-project-id\":\n    raise ValueError(\"Please set TRISMIK_PROJECT_ID. Create a project at https://app.trismik.com\")\n\nprint(f\"âœ“ Using project: {TRISMIK_PROJECT_ID}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Upload score() Results\n",
    "\n",
    "Score pre-computed outputs and upload to Trismik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare items with pre-computed outputs\nitems = [\n    {\"input\": \"What is 2 + 2?\", \"output\": \"4\", \"label\": \"4\"},\n    {\"input\": \"What is the capital of France?\", \"output\": \"Paris\", \"label\": \"Paris\"},\n    {\"input\": \"Who wrote Romeo and Juliet?\", \"output\": \"William Shakespeare\", \"label\": \"William Shakespeare\"},\n    {\"input\": \"What is 5 * 6?\", \"output\": \"30\", \"label\": \"30\"},\n    {\"input\": \"What is the largest planet?\", \"output\": \"Jupiter\", \"label\": \"Jupiter\"},\n]\n\n# Score and upload\nresults = score(\n    items=items,\n    metrics=Accuracy,\n    dataset_name=\"basic_questions\",\n    model_name=\"example-model-v1\",\n    experiment_id=\"Score-Upload-Notebook\",\n    project_id=TRISMIK_PROJECT_ID,\n    metadata={\n        \"description\": \"Example from Jupyter notebook\",\n        \"note\": \"Pre-computed outputs uploaded via score()\",\n    },\n    upload_results=True,  # Enable uploading\n)\n\nprint(f\"\\nâœ“ Results uploaded successfully!\")\nprint(f\"Accuracy: {results['aggregates']['Accuracy']:.2%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Upload evaluate() Results\n",
    "\n",
    "Run inference and automatically upload results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from typing import Any, List\nfrom scorebook import EvalDataset, evaluate\n\n# Create a simple dataset\nimport json\nfrom pathlib import Path\n\nsample_data = [\n    {\"question\": \"What is 10 + 5?\", \"answer\": \"15\"},\n    {\"question\": \"What is the capital of Spain?\", \"answer\": \"Madrid\"},\n]\n\ntemp_file = Path(\"temp_eval_dataset.json\")\nwith open(temp_file, \"w\") as f:\n    json.dump(sample_data, f)\n\ndataset = EvalDataset.from_json(\n    path=str(temp_file),\n    metrics=\"accuracy\",\n    input=\"question\",\n    label=\"answer\",\n)\n\n# Define a simple inference function (mock)\ndef mock_inference(inputs: List[Any], **hyperparameters: Any) -> List[Any]:\n    \"\"\"Mock inference that returns the expected answers.\"\"\"\n    # In practice, this would call your model\n    return [\"15\", \"Madrid\"]  # Mock perfect answers\n\n# Run evaluation with upload\neval_results = evaluate(\n    mock_inference,\n    dataset,\n    hyperparameters={\"temperature\": 0.7},\n    experiment_id=\"Evaluate-Upload-Notebook\",\n    project_id=TRISMIK_PROJECT_ID,\n    metadata={\n        \"model\": \"mock-model\",\n        \"description\": \"Evaluation results from notebook\",\n    },\n    return_aggregates=True,\n    return_items=True,\n    return_output=True,\n)\n\nprint(f\"\\nâœ“ Evaluation results uploaded!\")\nprint(f\"Accuracy: {eval_results['aggregates']['accuracy']:.2%}\")\n\n# Cleanup\ntemp_file.unlink()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Upload External Results\n",
    "\n",
    "Import results from external evaluation frameworks or historical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Import results from another evaluation framework\nexternal_results = [\n    {\"input\": \"Translate 'hello' to Spanish\", \"output\": \"hola\", \"label\": \"hola\"},\n    {\"input\": \"Translate 'goodbye' to Spanish\", \"output\": \"adiÃ³s\", \"label\": \"adiÃ³s\"},\n    {\"input\": \"Translate 'thank you' to Spanish\", \"output\": \"gracias\", \"label\": \"gracias\"},\n    {\"input\": \"Translate 'please' to Spanish\", \"output\": \"por favor\", \"label\": \"por favor\"},\n]\n\n# Upload external results\nexternal_upload = score(\n    items=external_results,\n    metrics=\"accuracy\",\n    dataset_name=\"spanish_translation\",\n    model_name=\"external-translator-v2\",\n    experiment_id=\"External-Results-Upload\",\n    project_id=TRISMIK_PROJECT_ID,\n    metadata={\n        \"description\": \"Historical results imported from external framework\",\n        \"source\": \"Custom evaluation pipeline\",\n        \"date\": \"2025-01-15\",\n    },\n    upload_results=True,\n)\n\nprint(f\"\\nâœ“ External results uploaded!\")\nprint(f\"Accuracy: {external_upload['aggregates']['accuracy']:.2%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results on Dashboard\n",
    "\n",
    "All uploaded results are now visible on your Trismik dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from IPython.display import display, Markdown\n\ndashboard_url = f\"https://app.trismik.com/projects/{TRISMIK_PROJECT_ID}\"\ndisplay(Markdown(f\"### ðŸ“Š [View All Results on Dashboard]({dashboard_url})\"))\nprint(f\"\\nDirect link: {dashboard_url}\")\nprint(\"\\nYou should see three experiments:\")\nprint(\"  1. Score-Upload-Notebook\")\nprint(\"  2. Evaluate-Upload-Notebook\")\nprint(\"  3. External-Results-Upload\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing Results with Metadata\n",
    "\n",
    "Use metadata to add context and organization to your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Organizing a model comparison experiment\nmodels_to_test = [\n    {\"name\": \"model-a\", \"version\": \"1.0\"},\n    {\"name\": \"model-b\", \"version\": \"2.0\"},\n]\n\ntest_items = [\n    {\"output\": \"positive\", \"label\": \"positive\"},\n    {\"output\": \"negative\", \"label\": \"negative\"},\n]\n\nfor model_info in models_to_test:\n    result = score(\n        items=test_items,\n        metrics=Accuracy,\n        dataset_name=\"sentiment_test\",\n        model_name=model_info[\"name\"],\n        experiment_id=\"Model-Comparison-Notebook\",\n        project_id=TRISMIK_PROJECT_ID,\n        metadata={\n            \"model_version\": model_info[\"version\"],\n            \"comparison_group\": \"sentiment_analysis\",\n            \"date\": \"2025-01-26\",\n            \"notes\": f\"Testing {model_info['name']} v{model_info['version']}\",\n        },\n        upload_results=True,\n    )\n    print(f\"âœ“ Uploaded results for {model_info['name']} v{model_info['version']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Experiment Naming\n",
    "- Use descriptive `experiment_id` values (e.g., \"GPT4-MMLU-Baseline\")\n",
    "- Group related runs under the same experiment ID\n",
    "- Use different experiment IDs for different types of tests\n",
    "\n",
    "### Metadata\n",
    "- Include model version, hyperparameters, and configuration\n",
    "- Add timestamps and descriptions for historical tracking\n",
    "- Use consistent keys across experiments for easy comparison\n",
    "\n",
    "### Organization\n",
    "- Create separate projects for different use cases\n",
    "- Use tags or metadata fields to categorize experiments\n",
    "- Document your evaluation methodology in metadata\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore the Trismik dashboard to visualize trends and comparisons\n",
    "- Set up automated evaluation pipelines with result uploading\n",
    "- Try the **Adaptive Evaluations** notebook for efficient testing with automatic uploads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}