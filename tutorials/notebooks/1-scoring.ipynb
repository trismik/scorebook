{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "g# Scoring Model Outputs with Scorebook\n",
    "\n",
    "This notebook demonstrates how to use Scorebook's `score()` function to evaluate pre-generated model predictions.\n",
    "\n",
    "## When to use `score()`\n",
    "\n",
    "- You already have model predictions and want to compute metrics\n",
    "- You want to re-score existing results with different metrics\n",
    "- You're importing evaluation results from another framework\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "from scorebook import score\n",
    "from scorebook.metrics import Accuracy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Your Data\n",
    "\n",
    "The `score()` function expects a list of items, where each item is a dictionary with:\n",
    "- `input`: The input to the model (optional, for reference)\n",
    "- `output`: The model's prediction\n",
    "- `label`: The ground truth answer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Example: Pre-generated model outputs\n",
    "items = [\n",
    "    {\n",
    "        \"input\": \"What is 2 + 2?\",\n",
    "        \"output\": \"4\",\n",
    "        \"label\": \"4\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": \"Paris\",\n",
    "        \"label\": \"Paris\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Who wrote Romeo and Juliet?\",\n",
    "        \"output\": \"William Shakespeare\",\n",
    "        \"label\": \"William Shakespeare\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is 5 * 6?\",\n",
    "        \"output\": \"30\",\n",
    "        \"label\": \"30\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the largest planet in our solar system?\",\n",
    "        \"output\": \"Jupiter\",\n",
    "        \"label\": \"Jupiter\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(items)} items for scoring\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the Results\n",
    "\n",
    "Now we'll use the `score()` function to compute accuracy metrics:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "results = score(\n",
    "    items=items,\n",
    "    metrics=Accuracy,\n",
    "    dataset_name=\"basic_questions\",\n",
    "    model_name=\"example-model\",\n",
    "    upload_results=False,  # Set to True to upload to Trismik\n",
    ")\n",
    "\n",
    "pprint(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "The results dictionary contains:\n",
    "- `aggregates`: Overall metrics (e.g., accuracy across all items)\n",
    "- `items`: Per-item scores and predictions\n",
    "- `metadata`: Information about the dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# View aggregate metrics\n",
    "print(\"\\nAggregate Metrics:\")\n",
    "print(f\"\\nAccuracy: {results['aggregate_results'][0]['accuracy']}\")\n",
    "\n",
    "# View per-item scores\n",
    "print(\"\\nPer-Item Scores:\")\n",
    "for i, item in enumerate(results['item_results'][:3], 1):\n",
    "    print(f\"\\nItem {i}:\")\n",
    "    print(f\"    Output: {item['output']}\")\n",
    "    print(f\"     Label: {item['label']}\")\n",
    "    print(f\"  Accuracy: {item['accuracy']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Next Steps\n\n- Try the **Evaluate** notebook to learn how to run inference and scoring together\n- See the **Upload Results** notebook to upload your scores to Trismik's dashboard\n- Explore custom metrics in the Scorebook documentation",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
