{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Scoring Model Outputs with Scorebook\n",
    "\n",
    "This notebook demonstrates how to use Scorebook's `score()` function to evaluate pre-computed model outputs.\n",
    "\n",
    "## When to use `score()`\n",
    "\n",
    "- You already have model outputs and want to compute metrics\n",
    "- You want to re-score existing results with different metrics\n",
    "- You're importing evaluation results from another framework\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pprint import pprint\n",
    "from scorebook import score\n",
    "from scorebook.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Your Data\n",
    "\n",
    "The `score()` function expects a list of items, where each item is a dictionary with:\n",
    "- `input`: The input to the model (optional, for reference)\n",
    "- `output`: The model's prediction\n",
    "- `label`: The ground truth answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Pre-computed model outputs\n",
    "items = [\n",
    "    {\n",
    "        \"input\": \"What is 2 + 2?\",\n",
    "        \"output\": \"4\",\n",
    "        \"label\": \"4\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": \"Paris\",\n",
    "        \"label\": \"Paris\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Who wrote Romeo and Juliet?\",\n",
    "        \"output\": \"William Shakespeare\",\n",
    "        \"label\": \"William Shakespeare\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is 5 * 6?\",\n",
    "        \"output\": \"30\",\n",
    "        \"label\": \"30\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the largest planet in our solar system?\",\n",
    "        \"output\": \"Jupiter\",\n",
    "        \"label\": \"Jupiter\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(items)} items for scoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the Results\n",
    "\n",
    "Now we'll use the `score()` function to compute accuracy metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = score(\n",
    "    items=items,\n",
    "    metrics=Accuracy,\n",
    "    dataset_name=\"basic_questions\",\n",
    "    model_name=\"example-model\",\n",
    "    upload_results=False,  # Set to True to upload to Trismik\n",
    ")\n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "The results dictionary contains:\n",
    "- `aggregates`: Overall metrics (e.g., accuracy across all items)\n",
    "- `items`: Per-item scores and predictions\n",
    "- `metadata`: Information about the dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View aggregate metrics\n",
    "print(\"\\nAggregate Metrics:\")\n",
    "print(f\"Accuracy: {results['aggregates']['Accuracy']:.2%}\")\n",
    "\n",
    "# View per-item scores\n",
    "print(\"\\nPer-Item Scores:\")\n",
    "for i, item in enumerate(results['items'][:3], 1):\n",
    "    print(f\"\\nItem {i}:\")\n",
    "    print(f\"  Output: {item['output']}\")\n",
    "    print(f\"  Label: {item['label']}\")\n",
    "    print(f\"  Accuracy: {item['Accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Multiple Metrics\n",
    "\n",
    "You can score with multiple metrics at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from scorebook.metrics import Precision\n\n# Binary classification example\nbinary_items = [\n    {\"output\": \"positive\", \"label\": \"positive\"},\n    {\"output\": \"negative\", \"label\": \"negative\"},\n    {\"output\": \"positive\", \"label\": \"negative\"},\n    {\"output\": \"negative\", \"label\": \"positive\"},\n]\n\nmulti_metric_results = score(\n    items=binary_items,\n    metrics=[Accuracy, Precision],\n    dataset_name=\"sentiment\",\n    model_name=\"example-classifier\",\n    upload_results=False,\n)\n\nprint(\"\\nMultiple Metrics Results:\")\nfor metric_name, value in multi_metric_results['aggregates'].items():\n    print(f\"{metric_name}: {value:.2%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Try the **Evaluate** notebook to learn how to run inference and scoring together\n",
    "- See the **Upload Results** notebook to upload your scores to Trismik's dashboard\n",
    "- Explore custom metrics in the Scorebook documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
