{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h9bv9vMWjhh"
      },
      "source": [
        "# Adaptive Testing with Scorebook and Local Inference\n",
        "\n",
        "This notebook demonstrates how to use Scorebook's adaptive evaluation capabilities with local transformer models.\n",
        "\n",
        "## What is Adaptive Testing?\n",
        "\n",
        "Adaptive testing dynamically adjusts question difficulty based on model performance, providing more efficient and accurate capability assessment.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "### Trismik\n",
        "\n",
        "To obtain a Trismik API key, go on https://www.trismik.com/ and click on Sign Up. You can start with a free account to test our platform!\n",
        "\n",
        "Once you're subscribed, log in the [dashboard](https://dashboard.trismik.com),\n",
        "click on your initials in the top-right corner of the screen, click on \"API Keys\" in the drop-down menu, and then on \"Create API Key\" to create a new API key. Copy-paste it in a text file - you will need it later in this tutorial.\n",
        "\n",
        "### Running on GPU\n",
        "\n",
        "If you're running this on a Google Colab, you can use a GPU for free by clicking on \"Runtime\" on the menu bar, selecting \"Change Runtime Type\" in the drop down menu, and then select \"T4 GPU\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ66qF8-Wjhi"
      },
      "source": [
        "## 1. Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM99RxQjWjhi"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -U scorebook \"tenacity<9.0.0\" transformers torch accelerate nest_asyncio -q\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpqL3lD3Wjhi"
      },
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8PZt2P1Wjhj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "from typing import Any, Dict, List\n",
        "from getpass import getpass\n",
        "import json\n",
        "import secrets\n",
        "\n",
        "import nest_asyncio\n",
        "import transformers\n",
        "import trismik\n",
        "from scorebook import evaluate, InferencePipeline, login\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set transformers verbosity\n",
        "transformers.utils.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-Nw1ecDWjhj"
      },
      "source": [
        "## 3. Authentication Setup\n",
        "\n",
        "Enter your Trismik credentials to enable adaptive evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE8fy8tXWjhj"
      },
      "outputs": [],
      "source": [
        "# Get Trismik credentials\n",
        "TRISMIK_API_KEY = getpass(\"Enter your Trismik API Key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82JGBKoMWjhj"
      },
      "outputs": [],
      "source": [
        "client = trismik.TrismikAsyncClient(api_key=TRISMIK_API_KEY)\n",
        "project_name = f\"Demo_Project_{secrets.token_hex(4)}\"\n",
        "project = await client.create_project(project_name)\n",
        "experiment_name = f\"Demo_Experiment_{secrets.token_hex(4)}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"You will find your experiment in the project {project_name} with the name {experiment_name}\")"
      ],
      "metadata": {
        "id": "TCvJ-ipFNMkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k5tLEiuWjhj"
      },
      "outputs": [],
      "source": [
        "# Login to Trismik\n",
        "login(TRISMIK_API_KEY)\n",
        "print(\"✓ Successfully logged in to Trismik\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmZJ0jmkWjhj"
      },
      "source": [
        "## 4. Load Local Model\n",
        "\n",
        "We'll use Microsoft's Phi-4-mini-instruct model for this example. You can replace this with any HuggingFace model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lw-MtEz3Wjhj"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"microsoft/Phi-4-mini-instruct\"\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "print(\"This may take a few minutes on first run...\")\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=MODEL_NAME,\n",
        "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(f\"✓ Model loaded successfully\")\n",
        "\n",
        "# Generation parameters for consistent outputs\n",
        "GENERATION_ARGS = {\n",
        "    \"max_new_tokens\": 10,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "    \"return_full_text\": False,\n",
        "    \"pad_token_id\": pipeline.tokenizer.eos_token_id\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7giFAYnWjhk"
      },
      "source": [
        "## 5. Define Inference Pipeline Components\n",
        "\n",
        "The InferencePipeline consists of three components:\n",
        "1. **Preprocessor**: Formats questions for the model\n",
        "2. **Inference**: Runs the model\n",
        "3. **Postprocessor**: Extracts answers from model output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAGj-AeLWjhk"
      },
      "outputs": [],
      "source": [
        "def preprocessor(eval_item: Dict, **hyperparameters: Any) -> List[Dict]:\n",
        "    \"\"\"Format evaluation item for HuggingFace model.\n",
        "\n",
        "    Args:\n",
        "        eval_item: Dictionary containing 'question' and optionally 'options'\n",
        "        hyperparameters: Additional configuration\n",
        "\n",
        "    Returns:\n",
        "        Messages formatted for the model\n",
        "    \"\"\"\n",
        "    # Build the prompt\n",
        "    prompt = eval_item[\"question\"]\n",
        "\n",
        "    # Add multiple choice options if present\n",
        "    if \"options\" in eval_item and eval_item[\"options\"]:\n",
        "        prompt += \"\\n\\nOptions:\\n\"\n",
        "        for i, option in enumerate(eval_item[\"options\"]):\n",
        "            letter = string.ascii_uppercase[i]\n",
        "            prompt += f\"{letter}: {option}\\n\"\n",
        "\n",
        "    # System instruction for clear answer - Phi-4 is a small model and needs\n",
        "    # a clear prompt\n",
        "    system_message = \"\"\"\n",
        "Answer the question you are given using only a single letter \\\n",
        "(for example, 'A'). \\\n",
        "Do not use punctuation. \\\n",
        "Do not show your reasoning. \\\n",
        "Do not provide any explanation. \\\n",
        "Follow the instructions exactly and \\\n",
        "always answer using a single uppercase letter.\n",
        "\n",
        "For example, if the question is \"What is the capital of France?\" and the \\\n",
        "choices are \"A. Paris\", \"B. London\", \"C. Rome\", \"D. Madrid\",\n",
        "- the answer should be \"A\"\n",
        "- the answer should NOT be \"Paris\" or \"A. Paris\" or \"A: Paris\"\n",
        "\n",
        "Please adhere strictly to the instructions.\n",
        "    \"\"\".strip()\n",
        "\n",
        "    # Format as messages for the model\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    return messages\n",
        "\n",
        "\n",
        "def inference(preprocessed_items: List[Any], **hyperparameters: Any) -> List[Any]:\n",
        "    \"\"\"Run inference on preprocessed items.\n",
        "\n",
        "    Args:\n",
        "        preprocessed_items: List of formatted messages\n",
        "        hyperparameters: Additional configuration\n",
        "\n",
        "    Returns:\n",
        "        List of model outputs\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for item in preprocessed_items:\n",
        "        try:\n",
        "            # Run inference with consistent generation parameters\n",
        "            output = pipeline(item, **GENERATION_ARGS)\n",
        "            results.append(output)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during inference: {e}\")\n",
        "            # Return empty output on error\n",
        "            results.append([{\"generated_text\": \"\"}])\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def postprocessor(model_output: Any, **hyperparameters: Any) -> str:\n",
        "    \"\"\"Extract answer from model output with retry logic.\n",
        "\n",
        "    Args:\n",
        "        model_output: Raw model output\n",
        "        hyperparameters: Additional configuration\n",
        "\n",
        "    Returns:\n",
        "        Extracted answer as a string\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract the generated text\n",
        "        if isinstance(model_output, list) and len(model_output) > 0:\n",
        "            generated = model_output[0].get(\"generated_text\", \"\")\n",
        "        else:\n",
        "            generated = str(model_output)\n",
        "\n",
        "        # Clean the response\n",
        "        answer = generated.strip()\n",
        "\n",
        "        # For single letter responses (multiple choice)\n",
        "        if len(answer) == 1 and answer in string.ascii_uppercase:\n",
        "            return answer\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in postprocessing: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Test the pipeline components\n",
        "print(\"Testing pipeline components...\")\n",
        "test_item = {\n",
        "    \"question\": \"What is 2+2?\",\n",
        "    \"options\": [\"3\", \"4\", \"5\", \"6\"]\n",
        "}\n",
        "\n",
        "test_preprocessed = preprocessor(test_item)\n",
        "print(f\"✓ Preprocessor test passed\")\n",
        "\n",
        "test_output = inference([test_preprocessed])\n",
        "print(f\"✓ Inference test passed\")\n",
        "\n",
        "test_answer = postprocessor(test_output[0])\n",
        "print(f\"✓ Postprocessor test passed\")\n",
        "print(f\"Test answer: '{test_answer}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1d6Z-23Wjhk"
      },
      "source": [
        "## 6. Create InferencePipeline and Run Adaptive Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7Ds9TRbWjhk"
      },
      "outputs": [],
      "source": [
        "# Create the InferencePipeline\n",
        "inference_pipeline = InferencePipeline(\n",
        "    model=MODEL_NAME,\n",
        "    preprocessor=preprocessor,\n",
        "    inference_function=inference,\n",
        "    postprocessor=postprocessor\n",
        ")\n",
        "\n",
        "print(\"✓ InferencePipeline created\")\n",
        "print(f\"Dataset: MMLUPro2025:adaptive\")\n",
        "print(f\"Experiment ID: {experiment_name}\")\n",
        "print(f\"Project ID: {project.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4W3tKwgWjhk"
      },
      "outputs": [],
      "source": [
        "print(\"\\nStarting adaptive evaluation...\")\n",
        "# Run the adaptive evaluation\n",
        "results = evaluate(\n",
        "    inference_pipeline,\n",
        "    datasets=\"MMLUPro2024:adaptive\",  # Adaptive dataset\n",
        "    experiment_id=experiment_name,\n",
        "    project_id=project.id,\n",
        "    return_dict=True,\n",
        "    return_aggregates=True,\n",
        "    return_items=True,\n",
        "    return_output=True\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Evaluation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaJzg-adWjhk"
      },
      "source": [
        "## 7. Analyze Results\n",
        "\n",
        "You can find the results in the `results` object returned by the `evaluate` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk8zSP3jWjhl"
      },
      "outputs": [],
      "source": [
        "results['aggregate_results'][0]['score']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyzing Results\n",
        "\n",
        "The key metrics to focus on are:\n",
        "\n",
        "- Theta (θ): The primary score measuring model ability on the dataset (higher is better)\n",
        "- Standard Error: The uncertainty in the theta estimate (lower is better)\n",
        "\n",
        "You can find more info [here](https://docs.trismik.com/adaptiveTesting/adaptive-testing-introduction/)!"
      ],
      "metadata": {
        "id": "AH0bdi9aX5gr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN991_RqWjhl"
      },
      "source": [
        "## Summary\n",
        "\n",
        "You've successfully run an adaptive evaluation using:\n",
        "- **Scorebook** for evaluation orchestration\n",
        "- **Local transformer models** for inference\n",
        "- **Trismik's adaptive datasets** for intelligent question selection\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Adaptive testing** adjusts difficulty based on model performance\n",
        "2. **Local inference** provides full control and no API costs\n",
        "3. **Scorebook's InferencePipeline** makes it easy to swap between local and cloud inference\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Try different models by changing `MODEL_NAME`\n",
        "- Adjust generation parameters for better accuracy\n",
        "- Explore other adaptive datasets available through Trismik"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
