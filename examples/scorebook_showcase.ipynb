{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ScoreBook Showcase\n",
    "This notebook demonstrates how to use Trismik's ScoreBook library to evaluate large language models. Scorebook is a library that allows you to evaluate LLMs with any dataset from Hugging Face or your own, and calculate scores for metrics such as accuracy, precision, recall, or F1. ScoreBook facilitates intuitive and efficient LLM experimentation with features such as grouping evaluations, batch inferencing, and sweeping across a grid of hyperparameter configurations."
   ],
   "id": "69bfc081dd8d2983"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Getting Started\n",
    "To show how ScoreBook can be used to easily evaluate a model of your choice by scoring it against a dataset. In this basic example we will use a model and dataset provided by Hugging Face."
   ],
   "id": "1d1b988ef78de372"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scorebook import EvalDataset, evaluate\n",
    "from scorebook.types.inference_pipeline import InferencePipeline\n",
    "import transformers\n",
    "\n",
    "# Create an evaluation dataset from any hugging face dataset by specifying its path, label field and split.\n",
    "mmlu_pro = EvalDataset.from_huggingface(\"TIGER-Lab/MMLU-Pro\", label=\"answer\", metrics=\"accuracy\", split=\"validation\")\n",
    "\n",
    "# In this example we use a simple Hugging Face text-generation pipeline for inference (use any compatible model you like).\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=\"microsoft/Phi-4-mini-instruct\")\n",
    "\n",
    "# Define pipeline components: preprocessor, inference function, and postprocessor\n",
    "def preprocessor(item: dict) -> str:\n",
    "    \"\"\"Convert evaluation item to model input format.\"\"\"\n",
    "    return item[\"question\"]\n",
    "\n",
    "def inference_function(processed_items: list[str], hyperparameters: dict) -> list:\n",
    "    \"\"\"Run model inference on preprocessed items.\"\"\"\n",
    "    outputs = [pipeline(item) for item in processed_items]\n",
    "    return outputs\n",
    "\n",
    "def postprocessor(output) -> str:\n",
    "    \"\"\"Extract the final answer from model output.\"\"\"\n",
    "    return output[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "# Create an inference pipeline that handles preprocessing, inference, and postprocessing\n",
    "inference_pipeline = InferencePipeline(\n",
    "    model=\"microsoft/Phi-4-mini-instruct\",\n",
    "    preprocessor=preprocessor,\n",
    "    inference_function=inference_function,\n",
    "    postprocessor=postprocessor,\n",
    ")\n",
    "\n",
    "# Run the evaluation: ScoreBook calls your inference pipeline, compares predictions to labels, and returns results.\n",
    "evaluation_results = evaluate(\n",
    "    inference_pipeline,  # the inference pipeline\n",
    "    mmlu_pro             # the evaluation dataset\n",
    ")"
   ],
   "id": "15c7512074eae2c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## ScoreBook Components\n",
    "When working with scorebook, there are 5 core components that should be considered and utilized:\n",
    "- Evaluation Datasets\n",
    "- Inference Pipelines\n",
    "- Metrics\n",
    "- The Evaluate Function\n",
    "- Evaluation Results\n",
    "\n",
    "The typical workflow for score book involves:\n",
    "1) Creating an evaluation dataset from local files of from hugging face\n",
    "2) Creating an inference pipeline responsible for returning a model output for each item in the evaluation dataset\n",
    "3) Assigning metrics to be used in scoring the model\n",
    "4) Using the `evaluate` function with a inference function, dataset, and metrics to generate scores"
   ],
   "id": "89bd176b3e8a093"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Evaluation Datasets\n",
    "\n",
    "Evaluation datasets are the foundation of model evaluation in ScoreBook. The `EvalDataset` class provides a unified interface for loading datasets from multiple sources and associating them with evaluation metrics.\n",
    "\n",
    "**Key Features:**\n",
    "- Load from HuggingFace Hub, CSV files, JSON files, or Python lists\n",
    "- Specify which field contains the ground truth labels\n",
    "- Associate evaluation metrics with the dataset\n",
    "- Built on top of HuggingFace datasets for compatibility\n",
    "\n",
    "**Supported Data Sources:**\n",
    "\n",
    "1. **HuggingFace Hub**: Load any public dataset\n",
    "```python\n",
    "dataset = EvalDataset.from_huggingface(\"TIGER-Lab/MMLU-Pro\", label=\"answer\", metrics=\"accuracy\")\n",
    "```\n",
    "2. **CSV Files**: Load from local CSV files\n",
    "```python\n",
    "dataset = EvalDataset.from_csv(\"data.csv\", label=\"ground_truth\", metrics=[\"accuracy\", \"precision\"])\n",
    "```\n",
    "3.  **JSON Files**: Support both flat and nested JSON structures\n",
    "```python\n",
    "dataset = EvalDataset.from_json(\"data.json\", label=\"answer\", metrics=\"accuracy\", split=\"test\")\n",
    "```"
   ],
   "id": "c24c09b16e7e4bf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scorebook import EvalDataset\n",
    "from scorebook.types.inference_pipeline import InferencePipeline\n",
    "\n",
    "# Create an evaluation dataset from any hugging face dataset by specifying its path, label field and split.\n",
    "mmlu_pro = EvalDataset.from_huggingface(\"TIGER-Lab/MMLU-Pro\", label=\"answer\", metrics=\"accuracy\", split=\"validation\")\n",
    "print(mmlu_pro)"
   ],
   "id": "7d24292e7a6ae31f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Inference Pipelines\n",
    "\n",
    "The `InferencePipeline` is ScoreBook's modular approach to model inference. It separates the inference process into three distinct, customizable stages, making it easy to work with different models and data formats.\n",
    "\n",
    "**Pipeline Stages:**\n",
    "\n",
    "1. **Pre-processor**: Converts dataset items into the format expected by your model\n",
    "2. **Inference Function**: Performs the actual model inference (can be sync or async)\n",
    "3. **Post-processor**: Extracts the final prediction from the model's raw output\n",
    "\n",
    "**Pre-processor**\n",
    "\n",
    "The preprocessor function within an inference pipeline, converts each item within an evaluation dataset, into the format expected by the module used. This may include transformations such as combining dataset fields, adding context, and formatting messages within a message dict structure.\n",
    "\n",
    "When writing a pre-processor function, it must accept a single evaluation dataset item as a parameter, and return an output that can be passed into a model\n",
    "\n",
    "Example Pre-Processor Function:\n",
    "```python\n",
    "def preprocessor(eval_item: dict) -> list:\n",
    "    \"\"\"Convert evaluation item to model input format.\"\"\"\n",
    "    prompt = f\"{eval_item['question']}\\nOptions:\\n\" + \"\\n\".join(\n",
    "        [f\"{letter} : {choice}\" for letter, choice in zip(string.ascii_uppercase, eval_item[\"options\"])]\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "                Answer the question you are given using only a single letter (for example, 'A').\n",
    "                Please adhere strictly to the instructions.\n",
    "            \"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "```\n",
    "\n",
    "**Inference Function**\n",
    "\n",
    "The inference function is responsible for taking a list of model inputs, generated by the pre-processor function, and returning a list of model outputs.\n",
    "\n",
    "**Postprocessor**\n",
    "\n",
    "The post-processor function returns a formatted response for each model input's output. This may include extracting the final generated message from a response containing model input, as well as string parsing.\n",
    "\n",
    "**Running an Inference Pipeline**\n",
    "\n",
    "**Example Implementation:**"
   ],
   "id": "3b1295a1c62ca0d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocessor(item: dict) -> str:\n",
    "    \"\"\"Convert evaluation item to model input format.\"\"\"\n",
    "    return f\"Question: {item['question']}\\nAnswer:\"\n",
    "\n",
    "def inference_function(processed_items: list[str], hyperparameters: dict) -> list:\n",
    "    \"\"\"Run model inference on preprocessed items.\"\"\"\n",
    "    # Use any model or API here - OpenAI, HuggingFace, local models, etc.\n",
    "    outputs = [model.generate(item, **hyperparameters) for item in processed_items]\n",
    "    return outputs\n",
    "\n",
    "def postprocessor(output) -> str:\n",
    "    \"\"\"Extract the final answer from model output.\"\"\"\n",
    "    return output.strip().split('\\n')[0]  # Get first line of response\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = InferencePipeline(\n",
    "    model=\"gpt-4\",\n",
    "    preprocessor=preprocessor,\n",
    "    inference_function=inference_function,\n",
    "    postprocessor=postprocessor,\n",
    ")"
   ],
   "id": "b863978ee3007943"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Metrics\n",
    "\n",
    "Metrics in ScoreBook quantify how well your model performs by comparing predictions against ground truth labels. The framework provides built-in metrics and supports custom metric creation.\n",
    "\n",
    "**Built-in Metrics:**\n",
    "\n",
    "1. **Accuracy**: Measures the percentage of correct predictions\n",
    "   ```python\n",
    "   from scorebook.metrics import Accuracy\n",
    "   metric = Accuracy()\n",
    "   aggregate_scores, item_scores = metric.score(outputs, labels)\n",
    "   ```\n",
    "\n",
    "2. **Precision**: Measures the accuracy of positive predictions\n",
    "   ```python\n",
    "   from scorebook.metrics import Precision\n",
    "   ```\n",
    "\n",
    "**Metric Architecture:**\n",
    "- All metrics inherit from `MetricBase` abstract class\n",
    "- Implement a `score()` method that returns both aggregate and per-item scores\n",
    "- Automatically registered via `@MetricRegistry.register()` decorator\n",
    "- Can be referenced by string name or class directly\n",
    "\n",
    "**Custom Metrics:**\n",
    "```python\n",
    "from scorebook.metrics import MetricBase, MetricRegistry\n",
    "\n",
    "@MetricRegistry.register()\n",
    "class CustomF1Score(MetricBase):\n",
    "    @staticmethod\n",
    "    def score(outputs, labels):\n",
    "        # Calculate F1 score logic here\n",
    "        item_scores = [calculate_f1(out, lab) for out, lab in zip(outputs, labels)]\n",
    "        aggregate_score = {\"f1\": sum(item_scores) / len(item_scores)}\n",
    "        return aggregate_score, item_scores\n",
    "```\n",
    "\n",
    "**Usage Patterns:**\n",
    "- Specify metrics when creating datasets: `metrics=[\"accuracy\", \"precision\"]`\n",
    "- Mix built-in and custom metrics: `metrics=[Accuracy, CustomF1Score]`\n",
    "- Access via registry: `MetricRegistry.get(\"accuracy\")`\n",
    "\n",
    "Each metric returns both aggregate scores (summary statistics) and item-level scores (individual predictions) for detailed analysis."
   ],
   "id": "1ddca9f75c35820e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluate\n\nThe `evaluate()` function is ScoreBook's central orchestrator that brings together datasets, inference pipelines, and metrics to produce evaluation results. It handles the entire evaluation workflow automatically.\n\n**Core Functionality:**\n\n```python\nfrom scorebook import evaluate\n\nresults = evaluate(\n    inference_pipeline,    # Your inference pipeline\n    eval_datasets,        # Single dataset or list of datasets  \n    hyperparameters=None, # Optional parameter sweep\n    item_limit=None,      # Limit evaluation to N items\n    score_type=\"aggregate\", # \"aggregate\", \"item\", or \"all\"\n    return_type=\"dict\"    # Format of returned results\n)\n```\n\n**Key Features:**\n\n1. **Multi-Dataset Support**: Evaluate on multiple datasets in one call\n   ```python\n   datasets = [mmlu_dataset, hellaswag_dataset, arc_dataset]\n   results = evaluate(pipeline, datasets)\n   ```\n\n2. **Hyperparameter Sweeping**: Test different model configurations\n   ```python\n   hyperparams = {\n       \"temperature\": [0.7, 0.9],\n       \"max_tokens\": [50, 100],\n       \"top_p\": [0.9, 0.95]\n   }\n   results = evaluate(pipeline, dataset, hyperparameters=hyperparams)\n   ```\n\n3. **Flexible Scoring**: Choose what level of detail you need\n   - `\"aggregate\"`: Overall dataset scores only\n   - `\"item\"`: Individual prediction scores only  \n   - `\"all\"`: Both aggregate and per-item scores\n\n4. **Progress Tracking**: Built-in progress bars for long evaluations\n\n5. **Async Support**: Handles both synchronous and asynchronous inference functions\n\n**Workflow:**\n1. Normalizes input datasets and expands hyperparameter grids\n2. For each dataset Ã— hyperparameter combination:\n   - Preprocesses items using the pipeline\n   - Runs model inference \n   - Postprocesses outputs\n   - Computes metric scores\n3. Formats and returns results according to specified parameters\n\nThe function returns structured results that can be easily analyzed, saved, or visualized.",
   "id": "7f8384b39c4a6bb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation Results\n\nScoreBook's evaluation results are structured data containers that organize and present your model's performance in multiple formats. The `EvalResult` class provides comprehensive access to both aggregate metrics and individual prediction details.\n\n**Result Structure:**\n\n```python\n# Results contain both aggregate and per-item information\nresults = evaluate(pipeline, dataset)\n\n# Access aggregate scores (overall performance)\naggregate = results[0][\"accuracy\"]  # Overall accuracy score\n\n# Access individual item results for detailed analysis  \nfor result in eval_result.item_scores:\n    print(f\"Item {result['item_id']}: {result['accuracy']}\")\n```\n\n**Key Properties:**\n\n1. **Aggregate Scores**: Summary statistics across the entire dataset\n   ```python\n   eval_result.aggregate_scores\n   # Returns: {\"dataset_name\": \"mmlu_pro\", \"accuracy\": 0.85, ...}\n   ```\n\n2. **Item Scores**: Per-prediction detailed results\n   ```python\n   eval_result.item_scores\n   # Returns list of: {\"item_id\": 0, \"dataset_name\": \"mmlu_pro\", \"accuracy\": True, ...}\n   ```\n\n3. **Export Capabilities**: Multiple output formats for analysis\n   ```python\n   # Save as structured JSON\n   eval_result.to_json(\"results.json\")\n   \n   # Export to CSV for spreadsheet analysis\n   eval_result.to_csv(\"results.csv\")\n   \n   # Get as dictionary for further processing\n   data = eval_result.to_dict()\n   ```\n\n**Hyperparameter Integration:**\nWhen using parameter sweeps, results automatically include hyperparameter information:\n```python\n{\n    \"dataset_name\": \"mmlu_pro\",\n    \"accuracy\": 0.87,\n    \"temperature\": 0.7,\n    \"max_tokens\": 100\n}\n```\n\n**Multi-Format Support:**\n- **Dictionary Format**: Easy programmatic access and JSON serialization\n- **CSV Format**: Spreadsheet-compatible for statistical analysis\n- **Structured Objects**: Rich Python objects with methods and properties\n\n**Analysis Workflow:**\n1. Run evaluation to get `EvalResult` objects\n2. Access aggregate scores for high-level performance overview  \n3. Drill into item scores to understand model behavior on specific examples\n4. Export results for visualization, reporting, or further analysis\n5. Compare results across different models, datasets, or hyperparameters",
   "id": "c80f155ec7caaf3e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
