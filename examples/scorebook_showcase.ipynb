{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bfc081dd8d2983",
   "metadata": {},
   "source": [
    "# ScoreBook Showcase\n",
    "This notebook demonstrates how to use Trismik's ScoreBook library to evaluate large language models (LLMs). ScoreBook defines clear contracts for data loading, inference, and metrics, allowing users to easily extend the library to support any dataset, inference function, or evaluation metric. Out of the box, ScoreBook supports a variety of Hugging Face datasets and pre-defined metrics, but users can customize it to fit their needs. The library facilitates intuitive and efficient LLM experimentation with features like grouping evaluations, batch inference, and hyperparameter grid sweeps, while handling orchestration and submission to the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b988ef78de372",
   "metadata": {},
   "source": [
    "---\n",
    "## Getting Started\n",
    "To show how ScoreBook can be used to easily evaluate a model of your choice by scoring it against a dataset. In this basic example we will use a model and simple example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c502f6d3e60d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c7512074eae2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/euan/Library/Caches/pypoetry/virtualenvs/scorebook-F74tV3fc-py3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 57.26it/s]\n",
      "Device set to use mps:0\n",
      "Datasets      0%|                                        | 0/1\n",
      "Hyperparams   0%|                                        | 0/1\u001b[A\n",
      "Hyperparams 100%|████████████████████████████████████████| 1/1\u001b[A\n",
      "Datasets    100%|████████████████████████████████████████| 1/1\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aggregate': [{'dataset_name': 'test_eval', 'accuracy': 0.0}], 'per_sample': [{'item_id': 0, 'dataset_name': 'test_eval', 'accuracy': False}, {'item_id': 1, 'dataset_name': 'test_eval', 'accuracy': False}, {'item_id': 2, 'dataset_name': 'test_eval', 'accuracy': False}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from scorebook import EvalDataset, evaluate\n",
    "from scorebook.types.inference_pipeline import InferencePipeline\n",
    "import transformers\n",
    "\n",
    "# Create an evaluation dataset from a list of dictionaries\n",
    "data = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"William Shakespeare\"}\n",
    "]\n",
    "\n",
    "# Create the evaluation dataset\n",
    "test_dataset = EvalDataset.from_list(\n",
    "    name=\"test_eval\",\n",
    "    label=\"answer\",\n",
    "    metrics=\"accuracy\",\n",
    "    data=data\n",
    ")\n",
    "\n",
    "# In this example we use a simple Hugging Face text-generation pipeline for inference (use any compatible model you like).\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=\"microsoft/Phi-4-mini-instruct\")\n",
    "\n",
    "def inference_function(eval_items: list[dict], **hyperparameters) -> list:\n",
    "    \"\"\"Direct inference function that handles preprocessing and postprocessing internally\"\"\"\n",
    "    results = []\n",
    "    for item in eval_items:\n",
    "\n",
    "        prompt = item[\"question\"]                           # 1) Preprocessing\n",
    "        output = pipeline(prompt)                           # 2) Inference\n",
    "        result = output[0][\"generated_text\"]                # 3) Postprocessing\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the evaluation: ScoreBook calls your inference pipeline, compares predictions to labels, and returns results.\n",
    "evaluation_results = evaluate(\n",
    "    inference_function,  # the inference function\n",
    "    test_dataset,        # the evaluation dataset\n",
    "    score_type = \"all\"\n",
    ")\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd176b3e8a093",
   "metadata": {},
   "source": [
    "---\n",
    "## ScoreBook Components\n",
    "When working with scorebook, there are 5 core components that should be considered and utilized:\n",
    "- Evaluation Datasets\n",
    "- Models\n",
    "- Metrics\n",
    "- The Evaluate Function\n",
    "- Evaluation Results\n",
    "\n",
    "The typical workflow for ScoreBook involves:\n",
    "1) Creating an evaluation dataset from local files of from hugging face\n",
    "2) Creating a callable responsible for returning a model's output for each item in the evaluation dataset\n",
    "3) Assigning metrics to be used in scoring the model\n",
    "4) Using the `evaluate` function with a model, dataset, and metrics to generate scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c09b16e7e4bf2",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation Datasets\n",
    "\n",
    "Evaluation datasets are the foundation of model evaluation in ScoreBook. The `EvalDataset` class provides a unified interface for loading datasets from multiple sources and associating them with evaluation metrics.\n",
    "\n",
    "**Key Features:**\n",
    "- Load from HuggingFace Hub, CSV files, JSON files, or Python lists\n",
    "- Specify which field contains the ground truth labels\n",
    "- Associate evaluation metrics with the dataset\n",
    "- Built on top of HuggingFace datasets for compatibility\n",
    "\n",
    "**Supported Data Sources:**\n",
    "\n",
    "1. **HuggingFace Hub**: Load any public dataset\n",
    "2. **CSV Files**: Load from local CSV files\n",
    "3. **JSON Files**: Support both flat and nested JSON structures\n",
    "4. **Python Lists**: Create an evaluation dataset from a list of dict objects, which represent field, value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d24292e7a6ae31f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:50:49.889111Z",
     "start_time": "2025-08-11T12:50:47.414626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<scorebook.types.eval_dataset.EvalDataset object at 0x163f8ad70>\n",
      "\n",
      "CSV Dataset: \n",
      "<scorebook.types.eval_dataset.EvalDataset object at 0x4effb6fd0>\n",
      "\n",
      "JSON Dataset: \n",
      "<scorebook.types.eval_dataset.EvalDataset object at 0x10644ef90>\n",
      "\n",
      "List Dataset: \n",
      "<scorebook.types.eval_dataset.EvalDataset object at 0x1640a2140>\n"
     ]
    }
   ],
   "source": [
    "# Load from HuggingFace Hub\n",
    "mmlu_pro = EvalDataset.from_huggingface(\"TIGER-Lab/MMLU-Pro\", label=\"answer\", metrics=\"accuracy\")\n",
    "print(mmlu_pro)\n",
    "\n",
    "# Example 2: Load from CSV (using the existing data.csv file)\n",
    "csv_dataset = EvalDataset.from_csv(\"data.csv\", label=\"answer\", metrics=[\"accuracy\"])\n",
    "print(f\"\\nCSV Dataset: \\n{csv_dataset}\")\n",
    "\n",
    "# Example 3: Load from JSON (using the existing data.json file)  \n",
    "json_dataset = EvalDataset.from_json(\"data.json\", label=\"answer\", metrics=\"accuracy\")\n",
    "print(f\"\\nJSON Dataset: \\n{json_dataset}\")\n",
    "\n",
    "# Example 4: Create from a Python list\n",
    "data = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"William Shakespeare\"}\n",
    "]\n",
    "list_dataset = EvalDataset.from_list(\"demo\", label=\"answer\", metrics=\"accuracy\", data=data)\n",
    "print(f\"\\nList Dataset: \\n{list_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519d66ae49ea555",
   "metadata": {},
   "source": [
    "---\n",
    "### Models\n",
    "\n",
    "To evaluate a model with ScoreBook, it must be represented by a single callable, which accepts a list of evaluation dataset items, and returns a list of parsed model outputs, ready to be scored against metrics.\n",
    "\n",
    "Within ScoreBook there are two structures that can be used to encapsulate this process:\n",
    "- **Inference Functions**: A single function which handles this entire process\n",
    "- **Inference Pipelines**: A callable `InferencePipeline` instance, which seperates the logic for pre-processing, inference, and post-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f64ab5ec33f10",
   "metadata": {},
   "source": [
    "---\n",
    "### Inference Functions\n",
    "\n",
    "An inference function is a single function responsible for generating a list of model outputs from a list of evaluation dataset items, to then be scored. Inference functions are responsible for the formatting of evaluation items into a structure that can be accepted by a model, generating a prediction for each model output, and parsing a model's output into results for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b939b41fb8642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from typing import Any, List, Dict\n",
    "\n",
    "\n",
    "def inference_function(eval_items: List[Dict], **hyperparameters: Any) -> List[Any]:\n",
    "    \"\"\"Pre-processes dataset items, inferencing, and post-processing result.\"\"\"\n",
    "    results = []\n",
    "    for eval_item in eval_items:\n",
    "\n",
    "        # For each evaluation item in an evaluation dataset, a prompt is created b combining multiple fields.\n",
    "        prompt = f\"{eval_item['question']}\\nOptions:\\n\" + \"\\n\".join(\n",
    "            [\n",
    "                f\"{letter} : {choice}\"\n",
    "                for letter, choice in zip(string.ascii_uppercase, eval_item[\"options\"])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # For each prompt created, it is structured into a message format with context ready to be passed into a model.\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "                    Answer the question you are given using only a single letter (for example, 'A').\n",
    "                        Do not use punctuation. \\\n",
    "                        Do not show your reasoning. \\\n",
    "                        Do not provide any explanation. \\\n",
    "                        Follow the instructions exactly and \\\n",
    "                        always answer using a single uppercase letter.\n",
    "\n",
    "                        For example, if the question is \"What is the capital of France?\" and the \\\n",
    "                        choices are \"A. Paris\", \"B. London\", \"C. Rome\", \"D. Madrid\",\n",
    "                        - the answer should be \"A\"\n",
    "                        - the answer should NOT be \"Paris\" or \"A. Paris\" or \"A: Paris\"\n",
    "\n",
    "                        Please adhere strictly to the instructions.\n",
    "                    \"\"\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "\n",
    "        # For each message, an output is generated, its content extracted, and it is appended to the list of results\n",
    "        output = pipeline(messages)\n",
    "        output = output[0][\"generated_text\"][-1][\"content\"]\n",
    "        results.append(output)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1295a1c62ca0d9",
   "metadata": {},
   "source": [
    "---\n",
    "### Inference Pipelines\n",
    "\n",
    "The `InferencePipeline` is ScoreBook's modular approach to model inference. It separates the inference process into three distinct, customizable stages, making it easy to work with different models and data formats.\n",
    "\n",
    "Inference pipelines provide a structured way to efficiently reuse pre-processing, inference, and post-processing logic. For instance, when pre-processing logic is tailored to the schema of a specific evaluation dataset, a new inference pipeline can be created for a model to evaluate a second dataset—without the need to rewrite the inference function or post-processing steps.\n",
    "\n",
    "**Pipeline Stages:**\n",
    "\n",
    "1. **Preprocessor**: Converts dataset items into the format expected by a model\n",
    "2. **Inference Function**: Performs the actual model inference (can be sync or async)\n",
    "3. **Postprocessor**: Extracts the final prediction from the model's raw output\n",
    "\n",
    "**Preprocessor**\n",
    "\n",
    "The preprocessor function within an inference pipeline converts each item within an evaluation dataset into the format expected by the model. This may include transformations such as combining dataset fields, adding context, and formatting messages within a message dict structure.\n",
    "\n",
    "When writing a preprocessor function, it must accept a single evaluation dataset item as a parameter and return an output that can be passed into a model.\n",
    "\n",
    "```python\n",
    "def preprocessor(eval_item: dict) -> list:\n",
    "    \"\"\"Convert evaluation item to model input format.\"\"\"\n",
    "    prompt = f\"{eval_item['question']}\\nOptions:\\n\" + \"\\n\".join(\n",
    "        [f\"{letter} : {choice}\" for letter, choice in zip(string.ascii_uppercase, eval_item[\"options\"])])\n",
    "\n",
    "    # The system message contains the instructions for the model. We ask the model to adhere strictly to the instructions\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "                Answer the question you are given using only a single letter (for example, 'A').\n",
    "            \"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    return messages\n",
    "```\n",
    "\n",
    "**Inference Function**\n",
    "\n",
    "The inference function is responsible for taking a list of model inputs, generated by the preprocessor function, and returning a list of model outputs.\n",
    "\n",
    "```python\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/Phi-4-mini-instruct\",\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "def inference_function(processed_items: list[list], **hyperparameters) -> list[Any]:\n",
    "    \"\"\"Run model inference on preprocessed items.\"\"\"\n",
    "    outputs = []\n",
    "    for messages in processed_items:\n",
    "        output = pipeline(messages)\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "```\n",
    "\n",
    "**Postprocessor**\n",
    "\n",
    "The postprocessor function returns a formatted response for each model input's output. This may include extracting the final generated message from a response containing model input, as well as string parsing.\n",
    "\n",
    "```python\n",
    "def postprocessor(model_output: Any) -> str:\n",
    "    \"\"\"Extract the final answer from model output.\"\"\"\n",
    "    return str(model_output[0][\"generated_text\"][-1][\"content\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863978ee3007943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic pipeline example\n",
    "def simple_preprocessor(item: dict) -> str:\n",
    "    \"\"\"Convert evaluation item to model input format.\"\"\"\n",
    "    return f\"Question: {item['question']}\\nAnswer:\"\n",
    "\n",
    "def mock_inference_function(processed_items: list[str], **hyperparameters) -> list:\n",
    "    \"\"\"Mock model inference - in practice, use your actual model here.\"\"\"\n",
    "    # This is a placeholder - replace with actual model calls\n",
    "    mock_outputs = [f\"Mock answer for: {item[:50]}...\" for item in processed_items]\n",
    "    return mock_outputs\n",
    "\n",
    "def simple_postprocessor(output) -> str:\n",
    "    \"\"\"Extract the final answer from model output.\"\"\"\n",
    "    return output.strip().split('\\n')[0]  # Get first line of response\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = InferencePipeline(\n",
    "    model=\"mock-model\",\n",
    "    preprocessor=simple_preprocessor,\n",
    "    inference_function=mock_inference_function,\n",
    "    postprocessor=simple_postprocessor,\n",
    ")\n",
    "\n",
    "print(\"Created inference pipeline with components:\")\n",
    "print(f\"- Model: {pipeline.model}\")\n",
    "print(\"- Preprocessor, inference function, and postprocessor configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddca9f75c35820e",
   "metadata": {},
   "source": [
    "---\n",
    "### Metrics\n",
    "\n",
    "Metrics in ScoreBook quantify how well your model performs by comparing predictions against ground truth labels. The framework provides built-in metrics and supports custom metric creation.\n",
    "\n",
    "**Built-in Metrics:**\n",
    "\n",
    "1. **Accuracy**: Measures the percentage of correct predictions\n",
    "2. **Precision**: Measures the accuracy of positive predictions (Not Implemented)\n",
    "\n",
    "**Metric Architecture:**\n",
    "- All metrics inherit from `MetricBase` abstract class\n",
    "- Implement a `score()` method that returns both aggregate and per-item scores\n",
    "- Automatically registered via `@MetricRegistry.register()` decorator\n",
    "- Can be referenced by string name or class directly\n",
    "\n",
    "**Usage Patterns:**\n",
    "- Specify metrics when creating datasets: `metrics=[\"accuracy\", \"precision\"]`\n",
    "- Mix built-in and custom metrics: `metrics=[Accuracy, CustomF1Score]`\n",
    "- Access via registry: `MetricRegistry.get(\"accuracy\")`\n",
    "\n",
    "Each metric returns both aggregate scores (summary statistics) and item-level scores (individual predictions) for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ttzgj0nrgul",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using built-in metrics\n",
    "from scorebook.metrics import Accuracy, Precision\n",
    "\n",
    "# Create metric instances\n",
    "accuracy_metric = Accuracy()\n",
    "precision_metric = Precision()\n",
    "\n",
    "# Example data\n",
    "outputs = [\"A\", \"B\", \"A\", \"C\", \"A\"]\n",
    "labels = [\"A\", \"A\", \"A\", \"C\", \"B\"]\n",
    "\n",
    "# Calculate accuracy scores\n",
    "acc_aggregate, acc_items = accuracy_metric.score(outputs, labels)\n",
    "print(\"Accuracy Results:\")\n",
    "print(f\"  Aggregate: {acc_aggregate}\")\n",
    "print(f\"  Per-item: {acc_items}\")\n",
    "\n",
    "# Calculate precision scores  \n",
    "prec_aggregate, prec_items = precision_metric.score(outputs, labels)\n",
    "print(\"\\nPrecision Results:\")\n",
    "print(f\"  Aggregate: {prec_aggregate}\")\n",
    "print(f\"  Per-item: {prec_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f1eee4580425d",
   "metadata": {},
   "source": [
    "---\n",
    "### Creating Custom Metrics\n",
    "\n",
    "New metrics can be created easily by defining a new metric class, that inherits from the `MetricBase` class and is registered in the metric registry with the `@MetricRegistry.register()` decorator. When creating a new metric, a score method must be defined which returns aggregate and item scores, calculated from a list of outputs and labels. The metric registry ensures that no two metrics can be defined with the same name, as well as facilitates the use of metric names as strings in the evaluate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hjnsibonbqp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Creating a custom exact match metric\n",
    "from scorebook.metrics import MetricBase, MetricRegistry\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "@MetricRegistry.register()\n",
    "class ExactMatchMetric(MetricBase):\n",
    "    \"\"\"Custom metric that checks for exact string matches.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def score(outputs: List[Any], labels: List[Any]) -> Tuple[Dict[str, Any], List[Any]]:\n",
    "        if len(outputs) != len(labels):\n",
    "            raise ValueError(\"Number of outputs must match number of labels\")\n",
    "            \n",
    "        # Calculate exact matches (case-sensitive)\n",
    "        item_scores = [str(output).strip() == str(label).strip() \n",
    "                      for output, label in zip(outputs, labels)]\n",
    "        \n",
    "        # Calculate aggregate score\n",
    "        exact_matches = sum(item_scores)\n",
    "        total = len(outputs)\n",
    "        aggregate_scores = {\"exact_match\": exact_matches / total if total > 0 else 0.0}\n",
    "        \n",
    "        return aggregate_scores, item_scores\n",
    "\n",
    "# Test the custom metric\n",
    "custom_metric = ExactMatchMetric()\n",
    "test_outputs = [\"Paris\", \"london\", \"Rome\", \"madrid\"]\n",
    "test_labels = [\"Paris\", \"London\", \"Rome\", \"Madrid\"]\n",
    "\n",
    "custom_agg, custom_items = custom_metric.score(test_outputs, test_labels)\n",
    "print(\"Custom ExactMatch Metric Results:\")\n",
    "print(f\"  Aggregate: {custom_agg}\")\n",
    "print(f\"  Per-item: {custom_items}\")\n",
    "\n",
    "# Access via registry\n",
    "registry_metric = MetricRegistry.get(\"exactmatchmetric\")\n",
    "print(f\"\\nMetric from registry: {registry_metric.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8384b39c4a6bb8",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluate\n",
    "\n",
    "The `evaluate()` function is ScoreBook's central orchestrator that brings together datasets, models, and metrics to produce evaluation results. It handles the entire evaluation workflow automatically.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Multi-Dataset Support**: Evaluate on multiple datasets in one call\n",
    "2. **Hyperparameter Sweeping**: Test different model configurations\n",
    "3. **Flexible Scoring**: Choose what level of detail you need\n",
    "   - `\"aggregate\"`: Overall dataset scores only\n",
    "   - `\"item\"`: Individual prediction scores only  \n",
    "   - `\"all\"`: Both aggregate and per-item scores\n",
    "4. **Progress Tracking**: Built-in progress bars for long evaluations\n",
    "5. **Async Support**: Handles both synchronous and asynchronous inference functions\n",
    "\n",
    "**Workflow:**\n",
    "1. Normalizes input datasets and expands hyperparameter grids\n",
    "2. For each dataset × hyperparameter combination:\n",
    "   - Preprocesses items using the pipeline\n",
    "   - Runs model inference \n",
    "   - Postprocesses outputs\n",
    "   - Computes metric scores\n",
    "3. Formats and returns results according to specified parameters\n",
    "\n",
    "The function returns structured results that can be easily analyzed, saved, or visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spy7fykt7l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic evaluate example using our previous components\n",
    "from scorebook import evaluate\n",
    "\n",
    "# Use the dataset and pipeline we created earlier\n",
    "demo_dataset = EvalDataset.from_list(\n",
    "    \"demo_eval\", \n",
    "    label=\"answer\", \n",
    "    metrics=[\"accuracy\"],\n",
    "    data=[\n",
    "        {\"question\": \"What is 2+2?\", \"answer\": \"4\"},\n",
    "        {\"question\": \"What is 3+3?\", \"answer\": \"6\"},\n",
    "        {\"question\": \"What is 5+5?\", \"answer\": \"10\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a simple mock inference pipeline for demonstration\n",
    "def demo_preprocessor(item: dict) -> str:\n",
    "    return item[\"question\"]\n",
    "\n",
    "def demo_inference(processed_items: list[str], **hyperparams) -> list[str]:\n",
    "    # Mock responses that partially match the expected answers\n",
    "    mock_responses = [\"4\", \"6\", \"wrong_answer\"]  # Third one is intentionally wrong\n",
    "    return mock_responses[:len(processed_items)]\n",
    "\n",
    "def demo_postprocessor(output: str) -> str:\n",
    "    return output.strip()\n",
    "\n",
    "demo_pipeline = InferencePipeline(\n",
    "    model=\"demo-model\",\n",
    "    preprocessor=demo_preprocessor,\n",
    "    inference_function=demo_inference,\n",
    "    postprocessor=demo_postprocessor\n",
    ")\n",
    "\n",
    "# Run basic evaluation\n",
    "results = evaluate(\n",
    "    demo_pipeline,\n",
    "    demo_dataset,\n",
    "    score_type=\"aggregate\"\n",
    ")\n",
    "\n",
    "print(\"Basic Evaluation Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4162e4ebe29041",
   "metadata": {},
   "source": [
    "---\n",
    "### Hyperparameter Sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37oum1dt0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Hyperparameter sweeping\n",
    "hyperparams = {\n",
    "    \"temperature\": [0.7, 0.9],\n",
    "    \"max_tokens\": [50, 100]\n",
    "}\n",
    "\n",
    "# Modified inference function that uses hyperparameters\n",
    "def param_aware_inference(processed_items: list[str], **hyperparams) -> list[str]:\n",
    "    temp = hyperparams.get(\"temperature\", 0.7)\n",
    "    max_tokens = hyperparams.get(\"max_tokens\", 50)\n",
    "    \n",
    "    # Mock different responses based on parameters\n",
    "    if temp > 0.8:\n",
    "        responses = [\"4\", \"6\", \"10\"]  # \"High temperature\" gives correct answers\n",
    "    else:\n",
    "        responses = [\"4\", \"7\", \"11\"]  # \"Low temperature\" gives some wrong answers\n",
    "        \n",
    "    return responses[:len(processed_items)]\n",
    "\n",
    "param_pipeline = InferencePipeline(\n",
    "    model=\"param-model\",\n",
    "    preprocessor=demo_preprocessor,\n",
    "    inference_function=param_aware_inference,\n",
    "    postprocessor=demo_postprocessor\n",
    ")\n",
    "\n",
    "# Run evaluation with hyperparameter sweep\n",
    "sweep_results = evaluate(\n",
    "    param_pipeline,\n",
    "    demo_dataset,\n",
    "    hyperparameters=hyperparams,\n",
    "    score_type=\"all\"\n",
    ")\n",
    "\n",
    "print(\"Hyperparameter Sweep Results:\")\n",
    "print(f\"Number of configurations tested: {len(sweep_results['aggregate'])}\")\n",
    "for i, result in enumerate(sweep_results['aggregate']):\n",
    "    print(f\"Config {i+1}: {result}\")\n",
    "\n",
    "# Show multi-dataset evaluation\n",
    "datasets = [demo_dataset, list_dataset]  # Use both datasets we created\n",
    "multi_results = evaluate(param_pipeline, datasets, score_type=\"aggregate\")\n",
    "print(f\"\\nMulti-dataset results: {len(multi_results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f155ec7caaf3e",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation Results\n",
    "\n",
    "The evaluate function returns results in customizable formats controlled by two optional parameters:\n",
    "  - return_type - Controls output format (dict or object)\n",
    "  - score_type - Controls which scores to include (aggregates or all)\n",
    "\n",
    "  Default Behavior\n",
    "  By default, evaluate returns a dictionary containing only aggregate scores `(return_type=\"dict\", score_type=\"aggregates\")`.\n",
    "\n",
    "  Dictionary Format\n",
    "  Returns aggregate scores as a simple dictionary structure.\n",
    "\n",
    "  Object Format\n",
    "  Set return_type=\"object\" to receive an EvalResult object with:\n",
    "  - `.aggregate_scores` - Aggregate scores as a flat dictionary\n",
    "  - `.item_scores` - Individual item scores as a flat dictionary\n",
    "\n",
    "  Both score dictionaries can be converted to pandas DataFrames.\n",
    "\n",
    "  Export Options\n",
    "  The EvalResult class provides convenient export methods:\n",
    "  - `.to_dict()` - Returns the same dictionary structure as the default format\n",
    "  - `.to_json()` - Saves scores to JSON file\n",
    "  - `.to_csv()` - Saves scores to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811efe91e933ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
