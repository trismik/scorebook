{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ScoreBook Showcase\n",
    "This notebook demonstrates how to use Trismik's ScoreBook library to evaluate large language models. Scorebook is a library that allows you to evaluate LLMs with any dataset from Hugging Face or your own, and calculate scores for metrics such as accuracy, precision, recall, or F1. ScoreBook facilitates intuitive and efficient LLM experimentation with features such as grouping evaluations, batch inferencing, and sweeping across a grid of hyperparameter configurations."
   ],
   "id": "69bfc081dd8d2983"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Getting Started\n",
    "To show how ScoreBook can be used to easily evaluate a model of your choice by scoring it against a dataset. In this basic example we will use a model and dataset provided by Hugging Face."
   ],
   "id": "1d1b988ef78de372"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scorebook import EvalDataset, evaluate\n",
    "import transformers\n",
    "\n",
    "# Create an evaluation dataset from any hugging face dataset by specifying its path, label field and split.\n",
    "mmlu_pro = EvalDataset.from_huggingface(\"TIGER-Lab/MMLU-Pro\", label=\"answer\", metrics=\"accuracy\", split=\"validation\")\n",
    "\n",
    "# In this example we use a simple Hugging Face text-generation pipeline for inference (use any compatible model you like).\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=\"microsoft/Phi-4-mini-instruct\")\n",
    "\n",
    "# Define an inference function for your model, which accepts a list of inputs, runs inference and returns a list of outputs.\n",
    "def inference(eval_items: list[dict]) -> list[str]:\n",
    "  outputs = [pipeline(item[\"question\"]) for item in eval_items]\n",
    "  inference_results = [output[0][\"generated_text\"][-1][\"content\"] for output in outputs]\n",
    "  return inference_results\n",
    "\n",
    "# Run the evaluation: ScoreBook calls your inference(), compares predictions to labels, and returns results.\n",
    "evaluation_results = evaluate(\n",
    "  inference,     # the inference function\n",
    "  mmlu_pro       # the evaluation dataset\n",
    "\n",
    ")"
   ],
   "id": "15c7512074eae2c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ee2d72a83b2571c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ScoreBook Components\n",
    "When working with scorebook, there are 5 core components that should be considered and utilized:\n",
    "- Evaluation Datasets\n",
    "- Inference Functions\n",
    "- Metrics\n",
    "- The Evaluate Function\n",
    "- Evaluation Results\n",
    "\n",
    "The typical workflow for score book involves:\n",
    "1) Creating an evaluation dataset from local files of from hugging face\n",
    "2) Creating an inference function responsible for returning a model output for each item in the evaluation dataset\n",
    "3) Assigning metrics to be used in scoring the model\n",
    "4) Using the `evaluate` function with a inference function, dataset, and metrics to generate scores"
   ],
   "id": "89bd176b3e8a093"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
