{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bfc081dd8d2983",
   "metadata": {},
   "source": [
    "# ScoreBook Showcase\n",
    "This notebook demonstrates how to use Trismik's ScoreBook library to evaluate large language models (LLMs). ScoreBook defines clear contracts for data loading, inference, and metrics, allowing users to easily extend the library to support any dataset, inference function, or evaluation metric. Out of the box, ScoreBook supports a variety of Hugging Face datasets and pre-defined metrics, but users can customize it to fit their needs. The library facilitates intuitive and efficient LLM experimentation with features like grouping evaluations, batch inference, and hyperparameter grid sweeps, while handling orchestration and submission to the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b988ef78de372",
   "metadata": {},
   "source": [
    "---\n",
    "## Getting Started\n",
    "To show how ScoreBook can be used to easily evaluate a model of your choice by scoring it against a dataset. In this basic example we will use a model and simple example dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "15c7512074eae2c4",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-10-22T20:52:03.858283Z",
     "start_time": "2025-10-22T20:51:44.305931Z"
    }
   },
   "source": [
    "from scorebook import EvalDataset, evaluate, InferencePipeline\n",
    "import transformers\n",
    "\n",
    "# Create an evaluation dataset from a list of dictionaries\n",
    "data = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"William Shakespeare\"}\n",
    "]\n",
    "\n",
    "# Create the evaluation dataset\n",
    "test_dataset = EvalDataset.from_list(\n",
    "    name=\"test_eval\",\n",
    "    input=\"question\",\n",
    "    label=\"answer\",\n",
    "    metrics=\"accuracy\",\n",
    "    items=data\n",
    ")\n",
    "\n",
    "# In this example we use a simple Hugging Face text-generation pipeline for inference (use any compatible model you like).\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=\"microsoft/Phi-4-mini-instruct\")\n",
    "\n",
    "def inference_function(inputs: list, **hyperparameters) -> list:\n",
    "    \"\"\"Direct inference function that handles preprocessing and postprocessing internally\"\"\"\n",
    "    results = []\n",
    "    for prompt in inputs:\n",
    "        output = pipeline(prompt)                           # Inference\n",
    "        result = output[0][\"generated_text\"]                # Postprocessing\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run the evaluation: ScoreBook calls your inference pipeline, compares predictions to labels, and returns results.\n",
    "evaluation_results = evaluate(\n",
    "    inference_function,  # the inference function\n",
    "    test_dataset,        # the evaluation dataset\n",
    "    return_items = True,  # returns both aggregate and item scores\n",
    "    upload_results = False\n",
    ")\n",
    "print(evaluation_results)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7505ca5999141988be55e079a4ed1f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "â ‹ Evaluating Model | 1 Dataset | 1 Hyperparam Configuration | 0/1 Runs   0%|          |"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b23dbff1c304957afb8bd4d749cd222"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aggregate_results': [{'dataset': 'test_eval', 'run_completed': True, 'accuracy': 0.0}], 'item_results': [{'id': 0, 'dataset_name': 'test_eval', 'input': 'What is 2 + 2?', 'label': '4', 'accuracy': False}, {'id': 1, 'dataset_name': 'test_eval', 'input': 'What is the capital of France?', 'label': 'Paris', 'accuracy': False}, {'id': 2, 'dataset_name': 'test_eval', 'input': 'Who wrote Romeo and Juliet?', 'label': 'William Shakespeare', 'accuracy': False}]}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "89bd176b3e8a093",
   "metadata": {},
   "source": [
    "---\n",
    "## ScoreBook Components\n",
    "When working with scorebook, there are 5 core components that should be considered and utilized:\n",
    "- Evaluation Datasets\n",
    "- Models\n",
    "- Metrics\n",
    "- The Evaluate Function\n",
    "- Evaluation Results\n",
    "\n",
    "The typical workflow for ScoreBook involves:\n",
    "1) Creating an evaluation dataset from local files of from hugging face\n",
    "2) Creating a callable responsible for returning a model's output for each item in the evaluation dataset\n",
    "3) Assigning metrics to be used in scoring the model\n",
    "4) Using the `evaluate` function with a model, dataset, and metrics to generate scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c09b16e7e4bf2",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation Datasets\n",
    "\n",
    "Evaluation datasets are the foundation of model evaluation in ScoreBook. The `EvalDataset` class provides a unified interface for loading datasets from multiple sources and associating them with evaluation metrics.\n",
    "\n",
    "**Key Features:**\n",
    "- Load from HuggingFace Hub, CSV files, JSON files, or Python lists\n",
    "- Specify which field contains the ground truth labels\n",
    "- Associate evaluation metrics with the dataset\n",
    "- Built on top of HuggingFace datasets for compatibility\n",
    "\n",
    "**Supported Data Sources:**\n",
    "\n",
    "1. **HuggingFace Hub**: Load any public dataset\n",
    "2. **CSV Files**: Load from local CSV files\n",
    "3. **JSON Files**: Support both flat and nested JSON structures\n",
    "4. **Python Lists**: Create an evaluation dataset from a list of dict objects, which represent field, value pairs."
   ]
  },
  {
   "cell_type": "code",
   "id": "7d24292e7a6ae31f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T20:52:09.314877Z",
     "start_time": "2025-10-22T20:52:03.871257Z"
    }
   },
   "source": [
    "from scorebook import EvalDataset\n",
    "\n",
    "# Load from HuggingFace Hub\n",
    "mmlu_pro = EvalDataset.from_huggingface(\"TIGER-Lab/MMLU-Pro\", input=\"question\", label=\"answer\", metrics=\"accuracy\")\n",
    "print(mmlu_pro)\n",
    "\n",
    "# Example 2: Load from CSV (using the existing dataset.csv file)\n",
    "csv_dataset = EvalDataset.from_csv(\"example_datasets/dataset.csv\", input=\"question\", label=\"answer\", metrics=[\"accuracy\"])\n",
    "print(f\"\\nCSV Dataset: \\n{csv_dataset}\")\n",
    "\n",
    "# Example 3: Load from JSON (using the existing dataset.json file)\n",
    "json_dataset = EvalDataset.from_json(\"example_datasets/dataset.json\", input=\"question\", label=\"answer\", metrics=\"accuracy\")\n",
    "print(f\"\\nJSON Dataset: \\n{json_dataset}\")\n",
    "\n",
    "# Example 4: Create from a Python list\n",
    "data = [\n",
    "    {\"question\": \"What is 2 + 2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote Romeo and Juliet?\", \"answer\": \"William Shakespeare\"}\n",
    "]\n",
    "list_dataset = EvalDataset.from_list(\"demo\", input=\"question\", label=\"answer\", metrics=\"accuracy\", items=data)\n",
    "print(f\"\\nList Dataset: \\n{list_dataset}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalDataset(\n",
      "  name='TIGER-Lab/MMLU-Pro',\n",
      "  rows=12032,\n",
      "  fields=[question_id, question, options, answer, answer_index, cot_content, category, src],\n",
      "  metrics=[accuracy],\n",
      "  input='question',\n",
      "  label='answer'\n",
      ")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File not found: example_datasets/dataset.csv",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(mmlu_pro)\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Example 2: Load from CSV (using the existing dataset.csv file)\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m csv_dataset = \u001B[43mEvalDataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mexample_datasets/dataset.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mquestion\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43manswer\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43maccuracy\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mCSV Dataset: \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mcsv_dataset\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# Example 3: Load from JSON (using the existing dataset.json file)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/scorebook/src/scorebook/eval_datasets/eval_dataset.py:271\u001B[39m, in \u001B[36mEvalDataset.from_csv\u001B[39m\u001B[34m(cls, path, metrics, input, label, name, encoding, newline, **reader_kwargs)\u001B[39m\n\u001B[32m    250\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Instantiate an EvalDataset from a CSV file.\u001B[39;00m\n\u001B[32m    251\u001B[39m \n\u001B[32m    252\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    268\u001B[39m \u001B[33;03m    MissingFieldError: If the input or label feature is not present in the first item.\u001B[39;00m\n\u001B[32m    269\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    270\u001B[39m reader_kwargs = reader_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n\u001B[32m--> \u001B[39m\u001B[32m271\u001B[39m validated_path = \u001B[43mvalidate_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpected_suffix\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    273\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    274\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(validated_path, encoding=encoding, newline=newline) \u001B[38;5;28;01mas\u001B[39;00m csvfile:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/scorebook/src/scorebook/utils/io_helpers.py:26\u001B[39m, in \u001B[36mvalidate_path\u001B[39m\u001B[34m(file_path, expected_suffix)\u001B[39m\n\u001B[32m     24\u001B[39m path = Path(file_path)\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m path.exists():\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFile not found: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m expected_suffix:\n\u001B[32m     29\u001B[39m     \u001B[38;5;66;03m# Convert single suffix to tuple for uniform handling\u001B[39;00m\n\u001B[32m     30\u001B[39m     allowed_suffixes = (\n\u001B[32m     31\u001B[39m         (expected_suffix,) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(expected_suffix, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m expected_suffix\n\u001B[32m     32\u001B[39m     )\n",
      "\u001B[31mFileNotFoundError\u001B[39m: File not found: example_datasets/dataset.csv"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "b519d66ae49ea555",
   "metadata": {},
   "source": [
    "---\n",
    "### Models\n",
    "\n",
    "To evaluate a model with ScoreBook, it must be represented by a single callable, which accepts a list of evaluation dataset items, and returns a list of parsed model outputs, ready to be scored against metrics.\n",
    "\n",
    "Within ScoreBook there are two structures that can be used to encapsulate this process:\n",
    "- **Inference Functions**: A single function which handles this entire process\n",
    "- **Inference Pipelines**: A callable `InferencePipeline` instance, which seperates the logic for pre-processing, inference, and post-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f64ab5ec33f10",
   "metadata": {},
   "source": [
    "---\n",
    "### Inference Functions\n",
    "\n",
    "An inference function is a single function responsible for generating a list of model outputs from a list of evaluation dataset items, to then be scored. Inference functions are responsible for the formatting of evaluation items into a structure that can be accepted by a model, generating a prediction for each model output, and parsing a model's output into results for scoring."
   ]
  },
  {
   "cell_type": "code",
   "id": "92b939b41fb8642c",
   "metadata": {},
   "source": [
    "import string\n",
    "from typing import Any, List\n",
    "\n",
    "\n",
    "def inference_function(inputs: List[str], **hyperparameters: Any) -> List[Any]:\n",
    "    \"\"\"Pre-processes dataset inputs, performs inference, and post-processes results.\"\"\"\n",
    "    results = []\n",
    "    for input_text in inputs:\n",
    "        # With the new API, inputs are already formatted strings\n",
    "        # The dataset template should have combined question and options into the input\n",
    "        \n",
    "        # Structure the input into a message format ready to be passed into a model\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "                    Answer the question you are given using only a single letter (for example, 'A').\n",
    "                        Do not use punctuation. \\\n",
    "                        Do not show your reasoning. \\\n",
    "                        Do not provide any explanation. \\\n",
    "                        Follow the instructions exactly and \\\n",
    "                        always answer using a single uppercase letter.\n",
    "\n",
    "                        For example, if the question is \"What is the capital of France?\" and the \\\n",
    "                        choices are \"A. Paris\", \"B. London\", \"C. Rome\", \"D. Madrid\",\n",
    "                        - the answer should be \"A\"\n",
    "                        - the answer should NOT be \"Paris\" or \"A. Paris\" or \"A: Paris\"\n",
    "\n",
    "                        Please adhere strictly to the instructions.\n",
    "                    \"\"\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": input_text},\n",
    "            ]\n",
    "\n",
    "        # For each message, an output is generated, its content extracted, and appended to results\n",
    "        output = pipeline(messages)\n",
    "        output = output[0][\"generated_text\"][-1][\"content\"]\n",
    "        results.append(output)\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b1295a1c62ca0d9",
   "metadata": {},
   "source": [
    "---\n",
    "### Inference Pipelines\n",
    "\n",
    "The `InferencePipeline` is ScoreBook's modular approach to model inference. It separates the inference process into three distinct, customizable stages, making it easy to work with different models and data formats.\n",
    "\n",
    "Inference pipelines provide a structured way to efficiently reuse pre-processing, inference, and post-processing logic. For instance, when pre-processing logic is tailored to a specific dataset, a new inference pipeline can be created for a different model or dataset without rewriting all components.\n",
    "\n",
    "**Pipeline Stages:**\n",
    "\n",
    "1. **Preprocessor**: Converts dataset input values into the format expected by a model\n",
    "2. **Inference Function**: Performs the actual model inference (can be sync or async)\n",
    "3. **Postprocessor**: Extracts the final prediction from the model's raw output\n",
    "\n",
    "**Preprocessor**\n",
    "\n",
    "The preprocessor function within an inference pipeline converts each input value from the evaluation dataset into the format expected by the model. This may include transformations such as adding context and formatting messages within a message dict structure.\n",
    "\n",
    "When writing a preprocessor function, it must accept a single input value (string) as a parameter and return an output that can be passed into a model.\n",
    "\n",
    "```python\n",
    "def preprocessor(input_value: str, **hyperparameters) -> list:\n",
    "    \"\"\"Convert evaluation input to model input format.\"\"\"\n",
    "    # For datasets with templates, input_value is already formatted\n",
    "    # Just structure it into messages for the model\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "                Answer the question you are given using only a single letter (for example, 'A').\n",
    "            \"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": input_value},\n",
    "    ]\n",
    "    return messages\n",
    "```\n",
    "\n",
    "**Inference Function**\n",
    "\n",
    "The inference function is responsible for taking a list of model inputs, generated by the preprocessor function, and returning a list of model outputs.\n",
    "\n",
    "```python\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/Phi-4-mini-instruct\",\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "def inference_function(processed_items: list[list], **hyperparameters) -> list[Any]:\n",
    "    \"\"\"Run model inference on preprocessed items.\"\"\"\n",
    "    outputs = []\n",
    "    for messages in processed_items:\n",
    "        output = pipeline(messages)\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "```\n",
    "\n",
    "**Postprocessor**\n",
    "\n",
    "The postprocessor function returns a formatted response for each model input's output. This may include extracting the final generated message from a response containing model input, as well as string parsing.\n",
    "\n",
    "```python\n",
    "def postprocessor(model_output: Any) -> str:\n",
    "    \"\"\"Extract the final answer from model output.\"\"\"\n",
    "    return str(model_output[0][\"generated_text\"][-1][\"content\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "b863978ee3007943",
   "metadata": {},
   "source": [
    "# Basic pipeline example\n",
    "def simple_preprocessor(input_value: str, **hyperparameters) -> str:\n",
    "    \"\"\"Convert evaluation input to model input format.\"\"\"\n",
    "    return f\"Question: {input_value}\\nAnswer:\"\n",
    "\n",
    "def mock_inference_function(processed_items: list[str], **hyperparameters) -> list:\n",
    "    \"\"\"Mock model inference - in practice, use your actual model here.\"\"\"\n",
    "    # This is a placeholder - replace with actual model calls\n",
    "    mock_outputs = [f\"Mock answer for: {item[:50]}...\" for item in processed_items]\n",
    "    return mock_outputs\n",
    "\n",
    "def simple_postprocessor(output) -> str:\n",
    "    \"\"\"Extract the final answer from the model output.\"\"\"\n",
    "    return output.strip().split('\\n')[0]  # Get the first line of the response\n",
    "\n",
    "# Create the pipeline\n",
    "from scorebook import InferencePipeline\n",
    "pipeline = InferencePipeline(\n",
    "    model=\"mock-model\",\n",
    "    preprocessor=simple_preprocessor,\n",
    "    inference_function=mock_inference_function,\n",
    "    postprocessor=simple_postprocessor,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ddca9f75c35820e",
   "metadata": {},
   "source": [
    "---\n",
    "### Metrics\n",
    "\n",
    "Metrics in ScoreBook quantify how well your model performs by comparing predictions against ground truth labels. The framework provides built-in metrics and supports custom metric creation.\n",
    "\n",
    "**Built-in Metrics:**\n",
    "\n",
    "1. **Accuracy**: Measures the percentage of correct predictions\n",
    "2. **Precision**: Measures the accuracy of positive predictions (Not Implemented)\n",
    "\n",
    "**Metric Architecture:**\n",
    "- All metrics inherit from `MetricBase` abstract class\n",
    "- Implement a `score()` method that returns both aggregate and per-item scores\n",
    "- Automatically registered via `@MetricRegistry.register()` decorator\n",
    "- Can be referenced by string name or class directly\n",
    "\n",
    "**Usage Patterns:**\n",
    "- Specify metrics when creating datasets: `metrics=[\"accuracy\", \"precision\"]`\n",
    "- Mix built-in and custom metrics: `metrics=[Accuracy, CustomF1Score]`\n",
    "- Access via registry: `MetricRegistry.get(\"accuracy\")`\n",
    "\n",
    "Each metric returns both aggregate scores (summary statistics) and item-level scores (individual predictions) for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "id": "ttzgj0nrgul",
   "metadata": {},
   "source": [
    "# Example: Using built-in metrics\n",
    "from scorebook.metrics import Accuracy, Precision\n",
    "\n",
    "# Create metric instances\n",
    "accuracy_metric = Accuracy()\n",
    "\n",
    "# Example data\n",
    "outputs = [\"A\", \"B\", \"A\", \"C\", \"A\"]\n",
    "labels  = [\"A\", \"A\", \"A\", \"C\", \"B\"]\n",
    "\n",
    "# Calculate accuracy scores\n",
    "acc_aggregate, acc_items = accuracy_metric.score(outputs, labels)\n",
    "print(\"Accuracy Results:\")\n",
    "print(f\"  Aggregate: {acc_aggregate}\")\n",
    "print(f\"  Per-item: {acc_items}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e95f1eee4580425d",
   "metadata": {},
   "source": [
    "---\n",
    "### Creating Custom Metrics\n",
    "\n",
    "New metrics can be created easily by defining a new metric class, that inherits from the `MetricBase` class and is registered in the metric registry with the `@MetricRegistry.register()` decorator. When creating a new metric, a score method must be defined which returns aggregate and item scores, calculated from a list of outputs and labels. The metric registry ensures that no two metrics can be defined with the same name, as well as facilitates the use of metric names as strings in the evaluate function."
   ]
  },
  {
   "cell_type": "code",
   "id": "hjnsibonbqp",
   "metadata": {},
   "source": [
    "# Example: Creating a custom exact match metric\n",
    "from scorebook.metrics import MetricBase, MetricRegistry\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "@MetricRegistry.register()\n",
    "class ExactMatchMetric(MetricBase):\n",
    "    \"\"\"Custom metric that checks for exact string matches.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def score(outputs: List[Any], labels: List[Any]) -> Tuple[Dict[str, Any], List[Any]]:\n",
    "        if len(outputs) != len(labels):\n",
    "            raise ValueError(\"Number of outputs must match number of labels\")\n",
    "            \n",
    "        # Calculate exact matches (case-sensitive)\n",
    "        item_scores = [str(output).strip() == str(label).strip() \n",
    "                      for output, label in zip(outputs, labels)]\n",
    "        \n",
    "        # Calculate aggregate score\n",
    "        exact_matches = sum(item_scores)\n",
    "        total = len(outputs)\n",
    "        aggregate_scores = {\"exact_match\": exact_matches / total if total > 0 else 0.0}\n",
    "        \n",
    "        return aggregate_scores, item_scores\n",
    "\n",
    "# Test the custom metric\n",
    "custom_metric = ExactMatchMetric()\n",
    "test_outputs = [\"Paris\", \"london\", \"Rome\", \"madrid\"]\n",
    "test_labels = [\"Paris\", \"London\", \"Rome\", \"Madrid\"]\n",
    "\n",
    "custom_agg, custom_items = custom_metric.score(test_outputs, test_labels)\n",
    "print(\"Custom ExactMatch Metric Results:\")\n",
    "print(f\"  Aggregate: {custom_agg}\")\n",
    "print(f\"  Per-item: {custom_items}\")\n",
    "\n",
    "# Access via registry\n",
    "registry_metric = MetricRegistry.get(\"exactmatchmetric\")\n",
    "print(f\"\\nMetric from registry: {registry_metric.name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f8384b39c4a6bb8",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluate\n",
    "\n",
    "The `evaluate()` function is ScoreBook's central orchestrator that brings together datasets, models, and metrics to produce evaluation results. It handles the entire evaluation workflow automatically.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "1. **Multi-Dataset Support**: Evaluate on multiple datasets in one call\n",
    "2. **Hyperparameter Sweeping**: Test different model configurations\n",
    "3. **Flexible Scoring**: Choose what level of detail you need\n",
    "   - `\"aggregate\"`: Overall dataset scores only\n",
    "   - `\"item\"`: Individual prediction scores only  \n",
    "   - `\"all\"`: Both aggregate and per-item scores\n",
    "4. **Progress Tracking**: Built-in progress bars for long evaluations\n",
    "5. **Async Support**: Handles both synchronous and asynchronous inference functions\n",
    "\n",
    "**Workflow:**\n",
    "1. Normalizes input datasets and expands hyperparameter grids\n",
    "2. For each dataset Ã— hyperparameter combination:\n",
    "   - Preprocesses items using the pipeline\n",
    "   - Runs model inference \n",
    "   - Postprocesses outputs\n",
    "   - Computes metric scores\n",
    "3. Formats and returns results according to specified parameters\n",
    "\n",
    "The function returns structured results that can be easily analyzed, saved, or visualized."
   ]
  },
  {
   "cell_type": "code",
   "id": "spy7fykt7l",
   "metadata": {},
   "source": [
    "# Basic evaluate example using our previous components\n",
    "from scorebook import evaluate, EvalDataset, InferencePipeline\n",
    "\n",
    "# Use the dataset and pipeline we created earlier\n",
    "demo_dataset = EvalDataset.from_list(\n",
    "    \"demo_eval\", \n",
    "    input=\"question\",\n",
    "    label=\"answer\", \n",
    "    metrics=[\"accuracy\"],\n",
    "    items=[\n",
    "        {\"question\": \"What is 2+2?\", \"answer\": \"4\"},\n",
    "        {\"question\": \"What is 3+3?\", \"answer\": \"6\"},\n",
    "        {\"question\": \"What is 5+5?\", \"answer\": \"10\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a simple mock inference pipeline for demonstration\n",
    "def demo_preprocessor(input_value: str, **hyperparameters) -> str:\n",
    "    return input_value\n",
    "\n",
    "def demo_inference(processed_items: list[str], **hyperparams) -> list[str]:\n",
    "    # Mock responses that partially match the expected answers\n",
    "    mock_responses = [\"4\", \"6\", \"wrong_answer\"]  # the third is intentionally wrong\n",
    "    return mock_responses[:len(processed_items)]\n",
    "\n",
    "def demo_postprocessor(output: str) -> str:\n",
    "    return output.strip()\n",
    "\n",
    "demo_pipeline = InferencePipeline(\n",
    "    model=\"demo-model\",\n",
    "    preprocessor=demo_preprocessor,\n",
    "    inference_function=demo_inference,\n",
    "    postprocessor=demo_postprocessor\n",
    ")\n",
    "\n",
    "# Run basic evaluation\n",
    "results = evaluate(\n",
    "    demo_pipeline,\n",
    "    demo_dataset,\n",
    ")\n",
    "\n",
    "print(\"Basic Evaluation Results:\")\n",
    "print(results)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4d4162e4ebe29041",
   "metadata": {},
   "source": [
    "---\n",
    "### Hyperparameter Sweeping\n",
    "\n",
    "  ScoreBook's hyperparameter sweeping feature allows you to systematically test different model configurations in a single evaluation run. This is essential for finding optimal\n",
    "   settings and understanding how hyperparameters affect model performance across datasets.\n",
    "\n",
    "  **How It Works:**\n",
    "  - Provide a dictionary where keys are hyperparameter names and values are lists of options to test\n",
    "  - ScoreBook automatically generates all possible combinations (Cartesian product)\n",
    "  - Each configuration is tested against your dataset(s) independently\n",
    "  - Results include performance metrics for every hyperparameter combination\n",
    "\n",
    "  **Key Benefits:**\n",
    "  - **Automated Grid Search**: No manual loops - ScoreBook handles all combinations\n",
    "  - **Progress Tracking**: Visual progress bars show evaluation status across configurations\n",
    "  - **Structured Results**: Easy comparison of performance across different settings\n",
    "  - **Multi-Dataset Support**: Test hyperparameter effects across multiple evaluation datasets simultaneously\n",
    "\n",
    "  **Common Use Cases:**\n",
    "  - Finding optimal temperature and token limits for text generation models\n",
    "  - Comparing sampling strategies (top-p, top-k, temperature combinations)\n",
    "  - Testing different prompt formatting or preprocessing approaches\n",
    "  - Evaluating model robustness across parameter ranges\n",
    "\n",
    "  The hyperparameters are passed directly to your inference function via `**hyperparameters`, allowing full control over how they're applied to your model."
   ]
  },
  {
   "cell_type": "code",
   "id": "d37oum1dt0b",
   "metadata": {},
   "source": [
    "# Example: Hyperparameter sweeping\n",
    "hyperparams = {\n",
    "    \"temperature\": [0.7, 0.9],\n",
    "    \"max_tokens\": [50, 100]\n",
    "}\n",
    "\n",
    "# Modified inference function that uses hyperparameters\n",
    "def param_aware_inference(processed_items: list[str], **hyperparams) -> list[str]:\n",
    "    temp = hyperparams.get(\"temperature\", 0.7)\n",
    "    max_tokens = hyperparams.get(\"max_tokens\", 50)\n",
    "    \n",
    "    # Mock different responses based on parameters\n",
    "    if temp > 0.8:\n",
    "        responses = [\"4\", \"6\", \"10\"]  # \"High temperature\" gives correct answers\n",
    "    else:\n",
    "        responses = [\"4\", \"7\", \"11\"]  # \"Low temperature\" gives some wrong answers\n",
    "        \n",
    "    return responses[:len(processed_items)]\n",
    "\n",
    "param_pipeline = InferencePipeline(\n",
    "    model=\"param-model\",\n",
    "    preprocessor=demo_preprocessor,\n",
    "    inference_function=param_aware_inference,\n",
    "    postprocessor=demo_postprocessor\n",
    ")\n",
    "\n",
    "# Run evaluation with hyperparameter sweep\n",
    "sweep_results = evaluate(\n",
    "    param_pipeline,\n",
    "    demo_dataset,\n",
    "    hyperparameters=hyperparams,\n",
    "    return_items=True\n",
    ")\n",
    "\n",
    "print(\"Hyperparameter Sweep Results:\")\n",
    "print(f\"Number of configurations tested: {len(sweep_results['aggregate'])}\")\n",
    "for i, result in enumerate(sweep_results['aggregate']):\n",
    "    print(f\"Config {i+1}: {result}\")\n",
    "\n",
    "# Show multi-dataset evaluation\n",
    "datasets = [demo_dataset, list_dataset]  # Use both datasets we created\n",
    "multi_results = evaluate(param_pipeline, datasets)\n",
    "print(f\"\\nMulti-dataset results: {len(multi_results)} results\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c80f155ec7caaf3e",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation Results\n",
    "\n",
    "The evaluate function returns results in customizable formats controlled by two optional parameters:\n",
    "  - return_type - Controls output format (dict or object)\n",
    "  - score_type - Controls which scores to include (aggregates or all)\n",
    "\n",
    "  Default Behavior\n",
    "  By default, evaluate returns a dictionary containing only aggregate scores `(return_type=\"dict\", score_type=\"aggregates\")`.\n",
    "\n",
    "  Dictionary Format\n",
    "  Returns aggregate scores as a simple dictionary structure.\n",
    "\n",
    "  Object Format\n",
    "  Set return_type=\"object\" to receive an EvalResult object with:\n",
    "  - `.aggregate_scores` - Aggregate scores as a flat dictionary\n",
    "  - `.item_scores` - Individual item scores as a flat dictionary\n",
    "\n",
    "  Both score dictionaries can be converted to pandas DataFrames.\n",
    "\n",
    "  Export Options\n",
    "  The EvalResult class provides convenient export methods:\n",
    "  - `.to_dict()` - Returns the same dictionary structure as the default format\n",
    "  - `.to_json()` - Saves scores to JSON file\n",
    "  - `.to_csv()` - Saves scores to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "id": "9811efe91e933ca0",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "aggregate_scores_df = pd.DataFrame(evaluation_results[\"aggregate_results\"])\n",
    "item_scores_df = pd.DataFrame(evaluation_results[\"item_results\"])\n",
    "\n",
    "print(\"\\nEvaluation Results:\\n\")\n",
    "print(f\"Aggregate Scores:\\n{aggregate_scores_df}\\n\")\n",
    "print(f\"Aggregate Scores:\\n{item_scores_df}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
