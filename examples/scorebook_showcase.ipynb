{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ScoreBook Showcase\n",
    "This notebook demonstrates how to use Trismik's ScoreBook library to evaluate large language models. Scorebook is a library that allows you to evaluate LLMs with any dataset from Hugging Face or your own, and calculate scores for metrics such as accuracy, precision, recall, or F1. ScoreBook facilitates intuitive and efficient LLM experimentation with features such as grouping evaluations, batch inferencing, and sweeping across a grid of hyperparameter configurations."
   ],
   "id": "69bfc081dd8d2983"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Getting Started\n",
    "To show how ScoreBook can be used to easily evaluate a model of your choice by scoring it against a dataset. In this basic example we will use a model and dataset provided by Hugging Face."
   ],
   "id": "1d1b988ef78de372"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from scorebook import EvalDataset, evaluate\nfrom scorebook.types.inference_pipeline import InferencePipeline\nimport transformers\n\n# Create an evaluation dataset from any hugging face dataset by specifying its path, label field and split.\nmmlu_pro = EvalDataset.from_huggingface(\"TIGER-Lab/MMLU-Pro\", label=\"answer\", metrics=\"accuracy\", split=\"validation\")\n\n# In this example we use a simple Hugging Face text-generation pipeline for inference (use any compatible model you like).\npipeline = transformers.pipeline(\"text-generation\", model=\"microsoft/Phi-4-mini-instruct\")\n\n# Define pipeline components: preprocessor, inference function, and postprocessor\ndef preprocessor(item: dict) -> str:\n    \"\"\"Convert evaluation item to model input format.\"\"\"\n    return item[\"question\"]\n\ndef inference_function(processed_items: list[str], **hyperparameters) -> list:\n    \"\"\"Run model inference on preprocessed items.\"\"\"\n    outputs = [pipeline(item) for item in processed_items]\n    return outputs\n\ndef postprocessor(output) -> str:\n    \"\"\"Extract the final answer from model output.\"\"\"\n    return output[0][\"generated_text\"][-1][\"content\"]\n\n# Create an inference pipeline that handles preprocessing, inference, and postprocessing\ninference_pipeline = InferencePipeline(\n    model=\"microsoft/Phi-4-mini-instruct\",\n    preprocessor=preprocessor,\n    inference_function=inference_function,\n    postprocessor=postprocessor,\n)\n\n# Run the evaluation: ScoreBook calls your inference pipeline, compares predictions to labels, and returns results.\nevaluation_results = evaluate(\n    inference_pipeline,  # the inference pipeline\n    mmlu_pro             # the evaluation dataset\n)",
   "id": "15c7512074eae2c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## ScoreBook Components\n",
    "When working with scorebook, there are 5 core components that should be considered and utilized:\n",
    "- Evaluation Datasets\n",
    "- Inference Pipelines\n",
    "- Metrics\n",
    "- The Evaluate Function\n",
    "- Evaluation Results\n",
    "\n",
    "The typical workflow for score book involves:\n",
    "1) Creating an evaluation dataset from local files of from hugging face\n",
    "2) Creating an inference pipeline responsible for returning a model output for each item in the evaluation dataset\n",
    "3) Assigning metrics to be used in scoring the model\n",
    "4) Using the `evaluate` function with a inference function, dataset, and metrics to generate scores"
   ],
   "id": "89bd176b3e8a093"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Evaluation Datasets\n",
    "\n",
    "Evaluation datasets are the foundation of model evaluation in ScoreBook. The `EvalDataset` class provides a unified interface for loading datasets from multiple sources and associating them with evaluation metrics.\n",
    "\n",
    "**Key Features:**\n",
    "- Load from HuggingFace Hub, CSV files, JSON files, or Python lists\n",
    "- Specify which field contains the ground truth labels\n",
    "- Associate evaluation metrics with the dataset\n",
    "- Built on top of HuggingFace datasets for compatibility\n",
    "\n",
    "**Supported Data Sources:**\n",
    "\n",
    "1. **HuggingFace Hub**: Load any public dataset\n",
    "2. **CSV Files**: Load from local CSV files\n",
    "3. **JSON Files**: Support both flat and nested JSON structures\n",
    "4. **Python Lists**:"
   ],
   "id": "c24c09b16e7e4bf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load from HuggingFace Hub\n",
    "mmlu_pro = EvalDataset.from_huggingface(\"TIGER-Lab/MMLU-Pro\", label=\"answer\", metrics=\"accuracy\")\n",
    "print(mmlu_pro)\n",
    "\n",
    "# Example 2: Load from CSV (would work if file existed)\n",
    "csv_dataset = EvalDataset.from_csv(\"data.csv\", label=\"ground_truth\", metrics=[\"accuracy\", \"precision\"])\n",
    "print(f\"\\nCSV Dataset: \\n{csv_dataset}\")\n",
    "\n",
    "# Example 3: Load from JSON (would work if file existed)  \n",
    "json_dataset = EvalDataset.from_json(\"data.json\", label=\"answer\", metrics=\"accuracy\", split=\"test\")\n",
    "print(f\"\\nJSON Dataset: \\n{json_dataset}\")\n",
    "\n",
    "# Example 4: Create from a Python list\n",
    "data = [\n",
    "    {\"question\": \"What is 2+2?\", \"answer\": \"4\"},\n",
    "    {\"question\": \"Capital of France?\", \"answer\": \"Paris\"}\n",
    "]\n",
    "list_dataset = EvalDataset.from_list(\"demo\", label=\"answer\", metrics=\"accuracy\", data=data)\n",
    "print(f\"List Dataset: \\n{list_dataset}\")"
   ],
   "id": "7d24292e7a6ae31f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---\n### Inference Pipelines\n\nThe `InferencePipeline` is ScoreBook's modular approach to model inference. It separates the inference process into three distinct, customizable stages, making it easy to work with different models and data formats.\n\n**Pipeline Stages:**\n\n1. **Preprocessor**: Converts dataset items into the format expected by your model\n2. **Inference Function**: Performs the actual model inference (can be sync or async)\n3. **Postprocessor**: Extracts the final prediction from the model's raw output\n\n**Preprocessor**\n\nThe preprocessor function within an inference pipeline converts each item within an evaluation dataset into the format expected by the model. This may include transformations such as combining dataset fields, adding context, and formatting messages within a message dict structure.\n\nWhen writing a preprocessor function, it must accept a single evaluation dataset item as a parameter and return an output that can be passed into a model.\n\n```python\ndef preprocessor(eval_item: dict) -> list:\n    \"\"\"Convert evaluation item to model input format.\"\"\"\n    prompt = f\"{eval_item['question']}\\nOptions:\\n\" + \"\\n\".join(\n        [f\"{letter} : {choice}\" for letter, choice in zip(string.ascii_uppercase, eval_item[\"options\"])])\n\n    # The system message contains the instructions for the model. We ask the model to adhere strictly to the instructions\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"\n                Answer the question you are given using only a single letter (for example, 'A').\n            \"\"\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    return messages\n```\n\n**Inference Function**\n\nThe inference function is responsible for taking a list of model inputs, generated by the preprocessor function, and returning a list of model outputs.\n\n```python\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=\"microsoft/Phi-4-mini-instruct\",\n    model_kwargs={\"torch_dtype\": \"auto\"},\n    device_map=\"auto\",\n)\n\ndef inference_function(processed_items: list[list], **hyperparameters) -> list[Any]:\n    \"\"\"Run model inference on preprocessed items.\"\"\"\n    outputs = []\n    for messages in processed_items:\n        output = pipeline(messages)\n        outputs.append(output)\n    return outputs\n```\n\n**Postprocessor**\n\nThe postprocessor function returns a formatted response for each model input's output. This may include extracting the final generated message from a response containing model input, as well as string parsing.\n\n```python\ndef postprocessor(model_output: Any) -> str:\n    \"\"\"Extract the final answer from model output.\"\"\"\n    return str(model_output[0][\"generated_text\"][-1][\"content\"])\n```\n\n**Key Benefits:**\n- **Flexibility**: Works with any model (OpenAI API, HuggingFace, local models)\n- **Reusability**: Components can be mixed and matched across different evaluations\n- **Batch Processing**: Handles multiple items efficiently\n- **Hyperparameter Support**: Pass model-specific parameters during evaluation",
   "id": "3b1295a1c62ca0d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Basic pipeline example\ndef simple_preprocessor(item: dict) -> str:\n    \"\"\"Convert evaluation item to model input format.\"\"\"\n    return f\"Question: {item['question']}\\nAnswer:\"\n\ndef mock_inference_function(processed_items: list[str], **hyperparameters) -> list:\n    \"\"\"Mock model inference - in practice, use your actual model here.\"\"\"\n    # This is a placeholder - replace with actual model calls\n    mock_outputs = [f\"Mock answer for: {item[:50]}...\" for item in processed_items]\n    return mock_outputs\n\ndef simple_postprocessor(output) -> str:\n    \"\"\"Extract the final answer from model output.\"\"\"\n    return output.strip().split('\\n')[0]  # Get first line of response\n\n# Create the pipeline\npipeline = InferencePipeline(\n    model=\"mock-model\",\n    preprocessor=simple_preprocessor,\n    inference_function=mock_inference_function,\n    postprocessor=simple_postprocessor,\n)\n\nprint(\"Created inference pipeline with components:\")\nprint(f\"- Model: {pipeline.model}\")\nprint(\"- Preprocessor, inference function, and postprocessor configured\")",
   "id": "b863978ee3007943"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---\n### Metrics\n\nMetrics in ScoreBook quantify how well your model performs by comparing predictions against ground truth labels. The framework provides built-in metrics and supports custom metric creation.\n\n**Built-in Metrics:**\n\n1. **Accuracy**: Measures the percentage of correct predictions\n2. **Precision**: Measures the accuracy of positive predictions\n\n**Metric Architecture:**\n- All metrics inherit from `MetricBase` abstract class\n- Implement a `score()` method that returns both aggregate and per-item scores\n- Automatically registered via `@MetricRegistry.register()` decorator\n- Can be referenced by string name or class directly\n\n**Usage Patterns:**\n- Specify metrics when creating datasets: `metrics=[\"accuracy\", \"precision\"]`\n- Mix built-in and custom metrics: `metrics=[Accuracy, CustomF1Score]`\n- Access via registry: `MetricRegistry.get(\"accuracy\")`\n\nEach metric returns both aggregate scores (summary statistics) and item-level scores (individual predictions) for detailed analysis.",
   "id": "1ddca9f75c35820e"
  },
  {
   "cell_type": "code",
   "id": "ttzgj0nrgul",
   "source": "# Example: Using built-in metrics\nfrom scorebook.metrics import Accuracy, Precision\n\n# Create metric instances\naccuracy_metric = Accuracy()\nprecision_metric = Precision()\n\n# Example data\noutputs = [\"A\", \"B\", \"A\", \"C\", \"A\"]\nlabels = [\"A\", \"A\", \"A\", \"C\", \"B\"]\n\n# Calculate accuracy scores\nacc_aggregate, acc_items = accuracy_metric.score(outputs, labels)\nprint(\"Accuracy Results:\")\nprint(f\"  Aggregate: {acc_aggregate}\")\nprint(f\"  Per-item: {acc_items}\")\n\n# Calculate precision scores  \nprec_aggregate, prec_items = precision_metric.score(outputs, labels)\nprint(\"\\nPrecision Results:\")\nprint(f\"  Aggregate: {prec_aggregate}\")\nprint(f\"  Per-item: {prec_items}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "hjnsibonbqp",
   "source": "# Example: Creating a custom metric\nfrom scorebook.metrics import MetricBase, MetricRegistry\nfrom typing import Any, Dict, List, Tuple\n\n@MetricRegistry.register()\nclass ExactMatchMetric(MetricBase):\n    \"\"\"Custom metric that checks for exact string matches.\"\"\"\n    \n    @staticmethod\n    def score(outputs: List[Any], labels: List[Any]) -> Tuple[Dict[str, Any], List[Any]]:\n        if len(outputs) != len(labels):\n            raise ValueError(\"Number of outputs must match number of labels\")\n            \n        # Calculate exact matches (case-sensitive)\n        item_scores = [str(output).strip() == str(label).strip() \n                      for output, label in zip(outputs, labels)]\n        \n        # Calculate aggregate score\n        exact_matches = sum(item_scores)\n        total = len(outputs)\n        aggregate_scores = {\"exact_match\": exact_matches / total if total > 0 else 0.0}\n        \n        return aggregate_scores, item_scores\n\n# Test the custom metric\ncustom_metric = ExactMatchMetric()\ntest_outputs = [\"Paris\", \"london\", \"Rome\", \"madrid\"]\ntest_labels = [\"Paris\", \"London\", \"Rome\", \"Madrid\"]\n\ncustom_agg, custom_items = custom_metric.score(test_outputs, test_labels)\nprint(\"Custom ExactMatch Metric Results:\")\nprint(f\"  Aggregate: {custom_agg}\")\nprint(f\"  Per-item: {custom_items}\")\n\n# Access via registry\nregistry_metric = MetricRegistry.get(\"exactmatchmetric\")\nprint(f\"\\nMetric from registry: {registry_metric.name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---\n### Evaluate\n\nThe `evaluate()` function is ScoreBook's central orchestrator that brings together datasets, inference pipelines, and metrics to produce evaluation results. It handles the entire evaluation workflow automatically.\n\n**Key Features:**\n\n1. **Multi-Dataset Support**: Evaluate on multiple datasets in one call\n2. **Hyperparameter Sweeping**: Test different model configurations\n3. **Flexible Scoring**: Choose what level of detail you need\n   - `\"aggregate\"`: Overall dataset scores only\n   - `\"item\"`: Individual prediction scores only  \n   - `\"all\"`: Both aggregate and per-item scores\n4. **Progress Tracking**: Built-in progress bars for long evaluations\n5. **Async Support**: Handles both synchronous and asynchronous inference functions\n\n**Workflow:**\n1. Normalizes input datasets and expands hyperparameter grids\n2. For each dataset Ã— hyperparameter combination:\n   - Preprocesses items using the pipeline\n   - Runs model inference \n   - Postprocesses outputs\n   - Computes metric scores\n3. Formats and returns results according to specified parameters\n\nThe function returns structured results that can be easily analyzed, saved, or visualized.",
   "id": "7f8384b39c4a6bb8"
  },
  {
   "cell_type": "code",
   "id": "spy7fykt7l",
   "source": "# Basic evaluate example using our previous components\nfrom scorebook import evaluate\n\n# Use the dataset and pipeline we created earlier\ndemo_dataset = EvalDataset.from_list(\n    \"demo_eval\", \n    label=\"answer\", \n    metrics=[\"accuracy\"],\n    data=[\n        {\"question\": \"What is 2+2?\", \"answer\": \"4\"},\n        {\"question\": \"What is 3+3?\", \"answer\": \"6\"},\n        {\"question\": \"What is 5+5?\", \"answer\": \"10\"}\n    ]\n)\n\n# Create a simple mock inference pipeline for demonstration\ndef demo_preprocessor(item: dict) -> str:\n    return item[\"question\"]\n\ndef demo_inference(processed_items: list[str], **hyperparams) -> list[str]:\n    # Mock responses that partially match the expected answers\n    mock_responses = [\"4\", \"6\", \"wrong_answer\"]  # Third one is intentionally wrong\n    return mock_responses[:len(processed_items)]\n\ndef demo_postprocessor(output: str) -> str:\n    return output.strip()\n\ndemo_pipeline = InferencePipeline(\n    model=\"demo-model\",\n    preprocessor=demo_preprocessor,\n    inference_function=demo_inference,\n    postprocessor=demo_postprocessor\n)\n\n# Run basic evaluation\nresults = evaluate(\n    demo_pipeline,\n    demo_dataset,\n    score_type=\"aggregate\"\n)\n\nprint(\"Basic Evaluation Results:\")\nprint(results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d37oum1dt0b",
   "source": "# Example: Hyperparameter sweeping\nhyperparams = {\n    \"temperature\": [0.7, 0.9],\n    \"max_tokens\": [50, 100]\n}\n\n# Modified inference function that uses hyperparameters\ndef param_aware_inference(processed_items: list[str], **hyperparams) -> list[str]:\n    temp = hyperparams.get(\"temperature\", 0.7)\n    max_tokens = hyperparams.get(\"max_tokens\", 50)\n    \n    # Mock different responses based on parameters\n    if temp > 0.8:\n        responses = [\"4\", \"6\", \"10\"]  # \"High temperature\" gives correct answers\n    else:\n        responses = [\"4\", \"7\", \"11\"]  # \"Low temperature\" gives some wrong answers\n        \n    return responses[:len(processed_items)]\n\nparam_pipeline = InferencePipeline(\n    model=\"param-model\",\n    preprocessor=demo_preprocessor,\n    inference_function=param_aware_inference,\n    postprocessor=demo_postprocessor\n)\n\n# Run evaluation with hyperparameter sweep\nsweep_results = evaluate(\n    param_pipeline,\n    demo_dataset,\n    hyperparameters=hyperparams,\n    score_type=\"all\"\n)\n\nprint(\"Hyperparameter Sweep Results:\")\nprint(f\"Number of configurations tested: {len(sweep_results['aggregate'])}\")\nfor i, result in enumerate(sweep_results['aggregate']):\n    print(f\"Config {i+1}: {result}\")\n\n# Show multi-dataset evaluation\ndatasets = [demo_dataset, list_dataset]  # Use both datasets we created\nmulti_results = evaluate(param_pipeline, datasets, score_type=\"aggregate\")\nprint(f\"\\nMulti-dataset results: {len(multi_results)} results\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---\n### Evaluation Results\n\nScoreBook's evaluation results are structured data containers that organize and present your model's performance in multiple formats. The `EvalResult` class provides comprehensive access to both aggregate metrics and individual prediction details.\n\n**Key Properties:**\n\n1. **Aggregate Scores**: Summary statistics across the entire dataset\n2. **Item Scores**: Per-prediction detailed results\n3. **Export Capabilities**: Multiple output formats for analysis\n\n**Multi-Format Support:**\n- **Dictionary Format**: Easy programmatic access and JSON serialization\n- **CSV Format**: Spreadsheet-compatible for statistical analysis\n- **Structured Objects**: Rich Python objects with methods and properties\n\n**Analysis Workflow:**\n1. Run evaluation to get `EvalResult` objects\n2. Access aggregate scores for high-level performance overview  \n3. Drill into item scores to understand model behavior on specific examples\n4. Export results for visualization, reporting, or further analysis\n5. Compare results across different models, datasets, or hyperparameters",
   "id": "c80f155ec7caaf3e"
  },
  {
   "cell_type": "code",
   "id": "j1e0gkmp8a",
   "source": "# Working with evaluation results\n# Run evaluation with detailed scoring\ndetailed_results = evaluate(\n    demo_pipeline,\n    demo_dataset, \n    score_type=\"all\"\n)\n\nprint(\"Result Structure:\")\nprint(f\"Keys: {list(detailed_results.keys())}\")\nprint(f\"Number of aggregate results: {len(detailed_results['aggregate'])}\")\nprint(f\"Number of per-sample results: {len(detailed_results['per_sample'])}\")\n\nprint(\"\\nAggregate Scores:\")\nfor result in detailed_results['aggregate']:\n    print(f\"  {result}\")\n\nprint(\"\\nPer-Sample Results:\")\nfor i, item in enumerate(detailed_results['per_sample'][:3]):  # Show first 3\n    print(f\"  Item {i}: {item}\")\n\n# Demonstrate accessing specific scores\nif detailed_results['aggregate']:\n    first_result = detailed_results['aggregate'][0]\n    accuracy_score = first_result.get('accuracy', 'N/A')\n    print(f\"\\nOverall Accuracy: {accuracy_score}\")\n\n# Show different score types\naggregate_only = evaluate(demo_pipeline, demo_dataset, score_type=\"aggregate\")\nitems_only = evaluate(demo_pipeline, demo_dataset, score_type=\"item\")\n\nprint(f\"\\nAggregate-only results: {len(aggregate_only)} items\")\nprint(f\"Items-only results: {len(items_only)} items\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
